#!/usr/bin/env python3
"""
ê¸°ë³¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ
CPU ìµœì í™”ëœ ê²½ëŸ‰ ì´ìƒ íƒì§€ AI
"""

import numpy as np
import librosa
import time
from scipy import signal
from scipy.signal import butter, filtfilt
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import joblib
import os
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import json
from collections import deque
import threading

class BasicAnomalyDetector:
    def __init__(self, 
                 model_save_path: str = "data/models/",
                 window_size: float = 5.0,
                 sample_rate: int = 16000):
        """
        ê¸°ë³¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            model_save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            window_size: ë¶„ì„ ìœˆë„ìš° í¬ê¸° (ì´ˆ)
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        """
        self.model_save_path = model_save_path
        os.makedirs(model_save_path, exist_ok=True)
        
        self.window_size = window_size
        self.sample_rate = sample_rate
        self.n_fft = 1024
        self.hop_length = 512
        
        # ëª¨ë¸ë“¤
        self.isolation_forest = None
        self.scaler = StandardScaler()
        self.pca = None
        self.is_trained = False
        
        # í†µê³„ ì •ë³´
        self.normal_stats = {}
        self.feature_names = [
            'rms_energy', 'spectral_centroid', 'spectral_rolloff', 
            'zero_crossing_rate', 'spectral_bandwidth', 'spectral_contrast',
            'low_freq_ratio', 'mid_freq_ratio', 'high_freq_ratio',
            'mfcc_1_mean', 'mfcc_2_mean', 'mfcc_3_mean'
        ]
        
        # ëª¨ë‹ˆí„°ë§ íˆìŠ¤í† ë¦¬
        self.detection_history = deque(maxlen=1000)
        self.performance_stats = {
            'total_detections': 0,
            'anomaly_count': 0,
            'average_processing_time': 0.0,
            'last_update': None
        }
        
        print("ğŸ§  ê¸°ë³¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"â±ï¸ ìœˆë„ìš° í¬ê¸°: {window_size}ì´ˆ")
        print(f"ğŸµ ìƒ˜í”Œë§ ë ˆì´íŠ¸: {sample_rate}Hz")
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_save_path}")
    
    def extract_features(self, audio_data: np.ndarray, sr: int) -> Dict[str, float]:
        """ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ (CPU ìµœì í™”)"""
        try:
            # ë¦¬ìƒ˜í”Œë§
            if sr != self.sample_rate:
                audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=self.sample_rate)
            
            # ë…¸ì´ì¦ˆ í•„í„°ë§
            nyquist = self.sample_rate / 2
            low_cutoff = 50 / nyquist
            high_cutoff = 4000 / nyquist
            b, a = butter(4, [low_cutoff, high_cutoff], btype='band')
            filtered_audio = filtfilt(b, a, audio_data)
            
            # 1. ì—ë„ˆì§€ íŠ¹ì§•
            rms_energy = np.sqrt(np.mean(filtered_audio ** 2))
            
            # 2. ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì§•
            stft = librosa.stft(filtered_audio, n_fft=self.n_fft, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            frequencies = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.n_fft)
            
            # ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì§•
            spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=filtered_audio, sr=self.sample_rate))
            spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=filtered_audio, sr=self.sample_rate))
            spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=filtered_audio, sr=self.sample_rate))
            spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=filtered_audio, sr=self.sample_rate))
            
            # Zero Crossing Rate
            zcr = np.mean(librosa.feature.zero_crossing_rate(filtered_audio, hop_length=self.hop_length))
            
            # ì£¼íŒŒìˆ˜ ëŒ€ì—­ë³„ ì—ë„ˆì§€ ë¹„ìœ¨
            low_freq_energy = np.sum(magnitude[(frequencies >= 50) & (frequencies <= 500), :])
            mid_freq_energy = np.sum(magnitude[(frequencies >= 500) & (frequencies <= 2000), :])
            high_freq_energy = np.sum(magnitude[(frequencies >= 2000) & (frequencies <= 4000), :])
            total_energy = np.sum(magnitude)
            
            if total_energy > 0:
                low_freq_ratio = low_freq_energy / total_energy
                mid_freq_ratio = mid_freq_energy / total_energy
                high_freq_ratio = high_freq_energy / total_energy
            else:
                low_freq_ratio = mid_freq_ratio = high_freq_ratio = 0
            
            # MFCC íŠ¹ì§• (ì²« 3ê°œë§Œ ì‚¬ìš©)
            mfccs = librosa.feature.mfcc(y=filtered_audio, sr=self.sample_rate, n_mfcc=13)
            mfcc_means = np.mean(mfccs, axis=1)
            
            features = {
                'rms_energy': float(rms_energy),
                'spectral_centroid': float(spectral_centroid),
                'spectral_rolloff': float(spectral_rolloff),
                'zero_crossing_rate': float(zcr),
                'spectral_bandwidth': float(spectral_bandwidth),
                'spectral_contrast': float(spectral_contrast),
                'low_freq_ratio': float(low_freq_ratio),
                'mid_freq_ratio': float(mid_freq_ratio),
                'high_freq_ratio': float(high_freq_ratio),
                'mfcc_1_mean': float(mfcc_means[0]),
                'mfcc_2_mean': float(mfcc_means[1]),
                'mfcc_3_mean': float(mfcc_means[2])
            }
            
            return features
            
        except Exception as e:
            print(f"âŒ íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {name: 0.0 for name in self.feature_names}
    
    def train_on_normal_data(self, normal_audio_files: List[str]) -> Dict:
        """
        ì •ìƒ ë°ì´í„°ë¡œ í›ˆë ¨
        
        Args:
            normal_audio_files: ì •ìƒ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í›ˆë ¨ ê²°ê³¼
        """
        print("ğŸ¯ ì •ìƒ ë°ì´í„°ë¡œ ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        print(f"ğŸ“ ì •ìƒ ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜: {len(normal_audio_files)}")
        
        # ì •ìƒ ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ
        normal_features = []
        processed_files = 0
        
        for i, file_path in enumerate(normal_audio_files):
            try:
                if i % 10 == 0:
                    print(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(normal_audio_files)}")
                
                audio_data, sr = librosa.load(file_path, sr=None)
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ì„
                chunk_samples = int(self.window_size * sr)
                for start in range(0, len(audio_data), chunk_samples):
                    chunk = audio_data[start:start + chunk_samples]
                    if len(chunk) >= chunk_samples:
                        features = self.extract_features(chunk, sr)
                        normal_features.append(list(features.values()))
                        processed_files += 1
                        
            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                continue
        
        if len(normal_features) < 10:
            raise ValueError("ì¶©ë¶„í•œ ì •ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 10ê°œ ìƒ˜í”Œ í•„ìš”")
        
        print(f"âœ… ì²˜ë¦¬ëœ ìƒ˜í”Œ ìˆ˜: {len(normal_features)}")
        
        # íŠ¹ì§• ì •ê·œí™”
        X_normal = np.array(normal_features)
        X_scaled = self.scaler.fit_transform(X_normal)
        
        # PCAë¡œ ì°¨ì› ì¶•ì†Œ (ë…¸ì´ì¦ˆ ê°ì†Œ)
        n_components = min(8, X_scaled.shape[1])
        self.pca = PCA(n_components=n_components, random_state=42)
        X_pca = self.pca.fit_transform(X_scaled)
        
        # Isolation Forest í›ˆë ¨
        contamination = 0.05  # 5%ë¥¼ ì´ìƒìœ¼ë¡œ ê°„ì£¼
        self.isolation_forest = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100,
            n_jobs=-1  # CPU ë³‘ë ¬ ì²˜ë¦¬
        )
        
        # ì •ìƒ ë°ì´í„°ë¡œ í›ˆë ¨
        self.isolation_forest.fit(X_pca)
        
        # ì •ìƒ ë°ì´í„° í†µê³„ ì €ì¥
        self._calculate_normal_statistics(X_normal)
        
        self.is_trained = True
        
        # í›ˆë ¨ ê²°ê³¼
        result = {
            'total_samples': len(normal_features),
            'processed_files': processed_files,
            'feature_dimensions': X_normal.shape[1],
            'pca_components': n_components,
            'explained_variance_ratio': float(np.sum(self.pca.explained_variance_ratio_)),
            'contamination_rate': contamination,
            'training_completed': True,
            'training_time': datetime.now().isoformat()
        }
        
        print("âœ… ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {result['total_samples']}")
        print(f"ğŸ“ˆ PCA ì„¤ëª… ë¶„ì‚°: {result['explained_variance_ratio']:.3f}")
        print(f"ğŸ¯ ì˜¤ì—¼ë¥ : {contamination}")
        
        return result
    
    def _calculate_normal_statistics(self, X_normal: np.ndarray):
        """ì •ìƒ ë°ì´í„° í†µê³„ ê³„ì‚°"""
        for i, feature_name in enumerate(self.feature_names):
            if i < X_normal.shape[1]:
                values = X_normal[:, i]
                self.normal_stats[feature_name] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'min': float(np.min(values)),
                    'max': float(np.max(values)),
                    'percentiles': {
                        'p5': float(np.percentile(values, 5)),
                        'p25': float(np.percentile(values, 25)),
                        'p75': float(np.percentile(values, 75)),
                        'p95': float(np.percentile(values, 95))
                    }
                }
    
    def detect_anomaly(self, audio_data: np.ndarray, sr: int) -> Dict:
        """
        ì´ìƒ íƒì§€ ìˆ˜í–‰
        
        Args:
            audio_data: ì˜¤ë””ì˜¤ ë°ì´í„°
            sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            
        Returns:
            ì´ìƒ íƒì§€ ê²°ê³¼
        """
        if not self.is_trained:
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': 'ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                'anomaly_type': 'model_not_trained',
                'processing_time_ms': 0
            }
        
        start_time = time.time()
        
        try:
            # íŠ¹ì§• ì¶”ì¶œ
            features = self.extract_features(audio_data, sr)
            feature_vector = np.array(list(features.values())).reshape(1, -1)
            
            # ì •ê·œí™” ë° PCA ë³€í™˜
            X_scaled = self.scaler.transform(feature_vector)
            X_pca = self.pca.transform(X_scaled)
            
            # Isolation Forest ì ìˆ˜ ê³„ì‚°
            isolation_score = self.isolation_forest.score_samples(X_pca)[0]
            is_anomaly = isolation_score < 0  # ìŒìˆ˜ë©´ ì´ìƒ
            
            # í†µê³„ ê¸°ë°˜ ì¶”ê°€ ê²€ì¦
            statistical_anomaly = self._check_statistical_anomaly(features)
            
            # ì´ìƒ ìœ í˜• ë¶„ë¥˜
            anomaly_type = self._classify_anomaly_type(features, statistical_anomaly)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(isolation_score, statistical_anomaly)
            
            # ìµœì¢… íŒì •
            final_anomaly = is_anomaly or statistical_anomaly
            
            processing_time = (time.time() - start_time) * 1000
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_performance_stats(final_anomaly, processing_time)
            
            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.detection_history.append({
                'timestamp': datetime.now(),
                'is_anomaly': final_anomaly,
                'confidence': confidence,
                'anomaly_type': anomaly_type,
                'isolation_score': float(isolation_score),
                'statistical_anomaly': statistical_anomaly,
                'processing_time_ms': processing_time
            })
            
            result = {
                'is_anomaly': final_anomaly,
                'confidence': confidence,
                'message': self._get_anomaly_message(anomaly_type, confidence),
                'anomaly_type': anomaly_type,
                'features': features,
                'isolation_score': float(isolation_score),
                'statistical_anomaly': statistical_anomaly,
                'processing_time_ms': processing_time
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ ì´ìƒ íƒì§€ ì˜¤ë¥˜: {e}")
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': f'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
                'anomaly_type': 'error',
                'processing_time_ms': (time.time() - start_time) * 1000
            }
    
    def _check_statistical_anomaly(self, features: Dict[str, float]) -> bool:
        """í†µê³„ ê¸°ë°˜ ì´ìƒ ê²€ì‚¬"""
        if not self.normal_stats:
            return False
        
        anomaly_count = 0
        total_features = 0
        
        for feature_name, value in features.items():
            if feature_name in self.normal_stats:
                stats = self.normal_stats[feature_name]
                if stats['std'] > 0:
                    # Z-score ê³„ì‚°
                    z_score = abs((value - stats['mean']) / stats['std'])
                    if z_score > 3:  # 3ì‹œê·¸ë§ˆ ê·œì¹™
                        anomaly_count += 1
                    total_features += 1
        
        # 30% ì´ìƒì˜ íŠ¹ì§•ì´ ì´ìƒì´ë©´ ì „ì²´ë¥¼ ì´ìƒìœ¼ë¡œ íŒì •
        return total_features > 0 and (anomaly_count / total_features) > 0.3
    
    def _classify_anomaly_type(self, features: Dict[str, float], 
                              statistical_anomaly: bool) -> str:
        """ì´ìƒ ìœ í˜• ë¶„ë¥˜"""
        if not statistical_anomaly:
            return 'normal'
        
        # ë² ì–´ë§ ë§ˆëª¨ (ê³ ì£¼íŒŒ, ZCR ë†’ìŒ)
        if (features.get('spectral_centroid', 0) > 2000 and 
            features.get('zero_crossing_rate', 0) > 0.2):
            return 'bearing_wear'
        
        # ì••ì¶•ê¸° ì´ìƒ (ì—ë„ˆì§€ íŒ¨í„´ ë³€í™”)
        if (features.get('rms_energy', 0) > 1.0 or 
            features.get('rms_energy', 0) < 0.01):
            return 'compressor_abnormal'
        
        # ëƒ‰ë§¤ ëˆ„ì¶œ (ì €ì£¼íŒŒ ì—ë„ˆì§€ ì¦ê°€)
        if features.get('low_freq_ratio', 0) > 0.8:
            return 'refrigerant_leak'
        
        # ì¼ë°˜ì ì¸ ì´ìƒ
        return 'general_anomaly'
    
    def _calculate_confidence(self, isolation_score: float, 
                            statistical_anomaly: bool) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        # Isolation Forest ì ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„
        score_confidence = min(1.0, max(0.0, abs(isolation_score) / 2.0))
        
        # í†µê³„ ê¸°ë°˜ ì‹ ë¢°ë„
        stat_confidence = 0.8 if statistical_anomaly else 0.2
        
        # ê°€ì¤‘ í‰ê· 
        confidence = 0.6 * score_confidence + 0.4 * stat_confidence
        return float(confidence)
    
    def _get_anomaly_message(self, anomaly_type: str, confidence: float) -> str:
        """ì´ìƒ ë©”ì‹œì§€ ìƒì„±"""
        messages = {
            'normal': 'ì •ìƒ ì‘ë™ ì¤‘',
            'bearing_wear': f'ë² ì–´ë§ ë§ˆëª¨ ì˜ì‹¬ (ì‹ ë¢°ë„: {confidence:.1%})',
            'compressor_abnormal': f'ì••ì¶•ê¸° ì´ìƒ ì˜ì‹¬ (ì‹ ë¢°ë„: {confidence:.1%})',
            'refrigerant_leak': f'ëƒ‰ë§¤ ëˆ„ì¶œ ì˜ì‹¬ (ì‹ ë¢°ë„: {confidence:.1%})',
            'general_anomaly': f'ì´ìƒ ì†ŒìŒ ê°ì§€ (ì‹ ë¢°ë„: {confidence:.1%})',
            'model_not_trained': 'ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',
            'error': 'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ'
        }
        return messages.get(anomaly_type, 'ì•Œ ìˆ˜ ì—†ëŠ” ì´ìƒ')
    
    def _update_performance_stats(self, is_anomaly: bool, processing_time: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.performance_stats['total_detections'] += 1
        
        if is_anomaly:
            self.performance_stats['anomaly_count'] += 1
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.performance_stats['total_detections']
        current_avg = self.performance_stats['average_processing_time']
        self.performance_stats['average_processing_time'] = (
            (current_avg * (total - 1) + processing_time) / total
        )
        
        self.performance_stats['last_update'] = datetime.now().isoformat()
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        stats = self.performance_stats.copy()
        
        if stats['total_detections'] > 0:
            stats['anomaly_rate'] = stats['anomaly_count'] / stats['total_detections']
        else:
            stats['anomaly_rate'] = 0.0
        
        return stats
    
    def get_detection_history(self, limit: int = 100) -> List[Dict]:
        """ìµœê·¼ íƒì§€ íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return list(self.detection_history)[-limit:]
    
    def save_model(self, filepath: str = None):
        """ëª¨ë¸ ì €ì¥"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if filepath is None:
            filepath = os.path.join(self.model_save_path, "basic_anomaly_detector.pkl")
        
        # ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ì €ì¥
        model_data = {
            'isolation_forest': self.isolation_forest,
            'scaler': self.scaler,
            'pca': self.pca,
            'normal_stats': self.normal_stats,
            'feature_names': self.feature_names,
            'window_size': self.window_size,
            'sample_rate': self.sample_rate,
            'is_trained': self.is_trained,
            'performance_stats': self.performance_stats
        }
        
        joblib.dump(model_data, filepath)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'basic_anomaly_detector',
            'created_at': datetime.now().isoformat(),
            'feature_count': len(self.feature_names),
            'pca_components': self.pca.n_components_ if self.pca else 0,
            'contamination_rate': 0.05,
            'window_size': self.window_size,
            'sample_rate': self.sample_rate
        }
        
        metadata_file = filepath.replace('.pkl', '_metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ê¸°ë³¸ ì´ìƒ íƒì§€ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_file}")
    
    def load_model(self, filepath: str = None):
        """ëª¨ë¸ ë¡œë“œ"""
        if filepath is None:
            filepath = os.path.join(self.model_save_path, "basic_anomaly_detector.pkl")
        
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        
        # ëª¨ë¸ ë¡œë“œ
        model_data = joblib.load(filepath)
        
        self.isolation_forest = model_data['isolation_forest']
        self.scaler = model_data['scaler']
        self.pca = model_data['pca']
        self.normal_stats = model_data['normal_stats']
        self.feature_names = model_data['feature_names']
        self.window_size = model_data['window_size']
        self.sample_rate = model_data['sample_rate']
        self.is_trained = model_data['is_trained']
        self.performance_stats = model_data.get('performance_stats', {})
        
        print(f"âœ… ê¸°ë³¸ ì´ìƒ íƒì§€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # ê¸°ë³¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    detector = BasicAnomalyDetector()
    
    print("ğŸ§  ê¸°ë³¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    print("CPU ìµœì í™”ëœ ê²½ëŸ‰ ì´ìƒ íƒì§€ AI")
    print("GPU ì—†ì´ë„ ê³ ì„±ëŠ¥ ë‹¬ì„± ê°€ëŠ¥")
    print("=" * 50)
    
    # ê°€ìƒì˜ ì •ìƒ ë°ì´í„°ë¡œ í›ˆë ¨ (ì‹¤ì œë¡œëŠ” íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)
    # normal_files = ["normal1.wav", "normal2.wav", ...]
    # training_result = detector.train_on_normal_data(normal_files)
    
    print("ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    print("ì •ìƒ ë°ì´í„°ë¡œ í›ˆë ¨ í›„ ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ ê°€ëŠ¥")
