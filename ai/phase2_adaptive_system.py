#!/usr/bin/env python3
"""
Phase 2: ì ì‘í˜• ì‹œìŠ¤í…œ êµ¬ì¶•
í™˜ê²½ ë³€í™”ì— ìë™ìœ¼ë¡œ ì ì‘í•˜ëŠ” AI ì‹œìŠ¤í…œ
"""

import numpy as np
import librosa
import time
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from collections import deque
import threading
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import joblib
import statistics

class Phase2AdaptiveSystem:
    def __init__(self, 
                 model_save_path: str = "data/models/phase2/",
                 window_size: float = 5.0,
                 sample_rate: int = 16000,
                 adaptation_interval: int = 100):  # 100ìƒ˜í”Œë§ˆë‹¤ ì ì‘
        """
        Phase 2 ì ì‘í˜• ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            model_save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            window_size: ë¶„ì„ ìœˆë„ìš° í¬ê¸° (ì´ˆ)
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            adaptation_interval: ì ì‘ ì—…ë°ì´íŠ¸ ê°„ê²© (ìƒ˜í”Œ ìˆ˜)
        """
        self.model_save_path = model_save_path
        os.makedirs(model_save_path, exist_ok=True)
        
        self.window_size = window_size
        self.sample_rate = sample_rate
        self.adaptation_interval = adaptation_interval
        
        # Phase 1 ì‹œìŠ¤í…œ (ê¸°ë³¸ ì´ìƒ íƒì§€)
        self.phase1_detector = None
        
        # ì ì‘í˜• ì‹œìŠ¤í…œë“¤
        self.adaptive_thresholds = {}
        self.online_learner = None
        self.cluster_analyzer = None
        
        # ì ì‘í˜• í†µê³„
        self.adaptation_stats = {
            'total_adaptations': 0,
            'last_adaptation': None,
            'adaptation_effectiveness': 0.0,
            'threshold_updates': 0,
            'model_updates': 0
        }
        
        # ì‹¤ì‹œê°„ ë°ì´í„° ë²„í¼
        self.data_buffer = deque(maxlen=1000)  # ìµœê·¼ 1000ê°œ ìƒ˜í”Œ
        self.performance_buffer = deque(maxlen=100)  # ìµœê·¼ 100ê°œ ì„±ëŠ¥ ì§€í‘œ
        
        # ì ì‘í˜• íŒŒë¼ë¯¸í„°
        self.adaptation_params = {
            'sensitivity': 0.1,  # ë¯¼ê°ë„ (0.0-1.0)
            'learning_rate': 0.01,  # í•™ìŠµë¥ 
            'confidence_threshold': 0.7,  # ì‹ ë¢°ë„ ì„ê³„ê°’
            'anomaly_threshold': 0.05,  # ì´ìƒ íƒì§€ ì„ê³„ê°’
            'adaptation_threshold': 0.1  # ì ì‘ íŠ¸ë¦¬ê±° ì„ê³„ê°’
        }
        
        # ì„±ëŠ¥ ì§€í‘œ
        self.performance_metrics = {
            'total_detections': 0,
            'anomaly_count': 0,
            'false_positives': 0,
            'false_negatives': 0,
            'average_processing_time': 0.0,
            'accuracy': 0.0,
            'adaptation_accuracy': 0.0,
            'last_update': None
        }
        
        print("ğŸ”„ Phase 2: ì ì‘í˜• ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"â±ï¸ ìœˆë„ìš° í¬ê¸°: {window_size}ì´ˆ")
        print(f"ğŸµ ìƒ˜í”Œë§ ë ˆì´íŠ¸: {sample_rate}Hz")
        print(f"ğŸ”„ ì ì‘ ê°„ê²©: {adaptation_interval}ìƒ˜í”Œ")
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_save_path}")
    
    def initialize_with_phase1(self, phase1_detector):
        """Phase 1 ì‹œìŠ¤í…œìœ¼ë¡œ ì´ˆê¸°í™”"""
        self.phase1_detector = phase1_detector
        
        # Phase 1ì˜ íŠ¹ì§• ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        self.feature_names = phase1_detector.feature_names
        
        # ì ì‘í˜• ì„ê³„ê°’ ì´ˆê¸°í™”
        self._initialize_adaptive_thresholds()
        
        # ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_online_learner()
        
        # í´ëŸ¬ìŠ¤í„° ë¶„ì„ê¸° ì´ˆê¸°í™”
        self._initialize_cluster_analyzer()
        
        print("âœ… Phase 1 ì‹œìŠ¤í…œìœ¼ë¡œ Phase 2 ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_adaptive_thresholds(self):
        """ì ì‘í˜• ì„ê³„ê°’ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.adaptive_thresholds = {}
        
        for feature_name in self.feature_names:
            self.adaptive_thresholds[feature_name] = {
                'lower': 0.0,
                'upper': 1.0,
                'mean': 0.0,
                'std': 1.0,
                'percentiles': {
                    'p5': 0.0,
                    'p25': 0.25,
                    'p75': 0.75,
                    'p95': 1.0
                },
                'update_count': 0,
                'last_update': None
            }
        
        print("ğŸ”„ ì ì‘í˜• ì„ê³„ê°’ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_online_learner(self):
        """ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.online_learner = {
            'isolation_forest': None,
            'scaler': StandardScaler(),
            'pca': None,
            'normal_stats': {},
            'anomaly_stats': {},
            'learning_rate': self.adaptation_params['learning_rate'],
            'sample_count': 0,
            'last_update': None
        }
        
        print("ğŸ§  ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_cluster_analyzer(self):
        """í´ëŸ¬ìŠ¤í„° ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.cluster_analyzer = {
            'kmeans': None,
            'cluster_centers': None,
            'cluster_labels': None,
            'anomaly_clusters': set(),
            'normal_clusters': set(),
            'cluster_stats': {},
            'last_analysis': None
        }
        
        print("ğŸ” í´ëŸ¬ìŠ¤í„° ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def detect_anomaly_adaptive(self, audio_data: np.ndarray, sr: int, 
                               ground_truth: Optional[bool] = None) -> Dict:
        """
        ì ì‘í˜• ì´ìƒ íƒì§€ ìˆ˜í–‰
        
        Args:
            audio_data: ì˜¤ë””ì˜¤ ë°ì´í„°
            sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            ground_truth: ì‹¤ì œ ì´ìƒ ì—¬ë¶€ (ì„±ëŠ¥ í‰ê°€ìš©)
            
        Returns:
            ì ì‘í˜• ì´ìƒ íƒì§€ ê²°ê³¼
        """
        if not self.phase1_detector or not self.phase1_detector.is_trained:
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': 'Phase 1 ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                'anomaly_type': 'system_not_initialized',
                'processing_time_ms': 0,
                'phase': 'Phase 2'
            }
        
        start_time = time.time()
        
        try:
            # 1. Phase 1 ê¸°ë³¸ íƒì§€
            phase1_result = self.phase1_detector.detect_anomaly(audio_data, sr, ground_truth)
            
            # 2. íŠ¹ì§• ì¶”ì¶œ
            features = self.phase1_detector.extract_enhanced_features(audio_data, sr)
            
            # 3. ì ì‘í˜• ì„ê³„ê°’ ê²€ì‚¬
            adaptive_anomaly = self._check_adaptive_thresholds(features)
            
            # 4. ì˜¨ë¼ì¸ í•™ìŠµ ê¸°ë°˜ íƒì§€
            online_anomaly = self._check_online_learning(features)
            
            # 5. í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê¸°ë°˜ íƒì§€
            cluster_anomaly = self._check_cluster_analysis(features)
            
            # 6. ë‹¤ì¤‘ ê²€ì¦ í†µí•©
            final_result = self._integrate_adaptive_predictions(
                phase1_result, adaptive_anomaly, online_anomaly, cluster_anomaly, features
            )
            
            # 7. ë°ì´í„° ë²„í¼ ì—…ë°ì´íŠ¸
            self._update_data_buffer(features, final_result['is_anomaly'], ground_truth)
            
            # 8. ì ì‘í˜• ì—…ë°ì´íŠ¸ (ì£¼ê¸°ì )
            if len(self.data_buffer) % self.adaptation_interval == 0:
                self._perform_adaptive_update()
            
            # 9. ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = (time.time() - start_time) * 1000
            self._update_performance_metrics(final_result, ground_truth, processing_time)
            
            final_result['processing_time_ms'] = processing_time
            final_result['phase'] = 'Phase 2'
            final_result['adaptation_stats'] = self.adaptation_stats.copy()
            
            return final_result
            
        except Exception as e:
            print(f"âŒ ì ì‘í˜• ì´ìƒ íƒì§€ ì˜¤ë¥˜: {e}")
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': f'ì ì‘í˜• ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
                'anomaly_type': 'error',
                'processing_time_ms': (time.time() - start_time) * 1000,
                'phase': 'Phase 2'
            }
    
    def _check_adaptive_thresholds(self, features: Dict[str, float]) -> Dict:
        """ì ì‘í˜• ì„ê³„ê°’ ê²€ì‚¬"""
        anomaly_scores = []
        anomaly_features = []
        
        for feature_name, value in features.items():
            if feature_name in self.adaptive_thresholds:
                threshold = self.adaptive_thresholds[feature_name]
                
                # Z-score ê³„ì‚°
                if threshold['std'] > 0:
                    z_score = abs((value - threshold['mean']) / threshold['std'])
                    anomaly_scores.append(z_score)
                    
                    # 3ì‹œê·¸ë§ˆ ê·œì¹™
                    if z_score > 3:
                        anomaly_features.append(feature_name)
        
        # ì „ì²´ ì´ìƒ ì ìˆ˜
        overall_anomaly_score = np.mean(anomaly_scores) if anomaly_scores else 0.0
        
        return {
            'is_anomaly': len(anomaly_features) > 2 or overall_anomaly_score > 2.5,
            'anomaly_score': overall_anomaly_score,
            'anomaly_features': anomaly_features,
            'confidence': min(1.0, overall_anomaly_score / 3.0)
        }
    
    def _check_online_learning(self, features: Dict[str, float]) -> Dict:
        """ì˜¨ë¼ì¸ í•™ìŠµ ê¸°ë°˜ íƒì§€"""
        if not self.online_learner['isolation_forest']:
            return {'is_anomaly': False, 'confidence': 0.0}
        
        try:
            # íŠ¹ì§• ë²¡í„° ë³€í™˜
            feature_vector = np.array(list(features.values())).reshape(1, -1)
            
            # ì •ê·œí™” ë° PCA ë³€í™˜
            X_scaled = self.online_learner['scaler'].transform(feature_vector)
            X_pca = self.online_learner['pca'].transform(X_scaled)
            
            # Isolation Forest ì ìˆ˜ ê³„ì‚°
            isolation_score = self.online_learner['isolation_forest'].score_samples(X_pca)[0]
            is_anomaly = isolation_score < 0
            
            return {
                'is_anomaly': is_anomaly,
                'confidence': min(1.0, abs(isolation_score) / 2.0),
                'isolation_score': isolation_score
            }
            
        except Exception as e:
            print(f"âŒ ì˜¨ë¼ì¸ í•™ìŠµ íƒì§€ ì˜¤ë¥˜: {e}")
            return {'is_anomaly': False, 'confidence': 0.0}
    
    def _check_cluster_analysis(self, features: Dict[str, float]) -> Dict:
        """í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê¸°ë°˜ íƒì§€"""
        if not self.cluster_analyzer['kmeans']:
            return {'is_anomaly': False, 'confidence': 0.0}
        
        try:
            # íŠ¹ì§• ë²¡í„° ë³€í™˜
            feature_vector = np.array(list(features.values())).reshape(1, -1)
            
            # í´ëŸ¬ìŠ¤í„° ì˜ˆì¸¡
            cluster_label = self.cluster_analyzer['kmeans'].predict(feature_vector)[0]
            
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê¹Œì§€ì˜ ê±°ë¦¬
            cluster_center = self.cluster_analyzer['cluster_centers'][cluster_label]
            distance = np.linalg.norm(feature_vector - cluster_center)
            
            # ì´ìƒ í´ëŸ¬ìŠ¤í„° ì—¬ë¶€ í™•ì¸
            is_anomaly_cluster = cluster_label in self.cluster_analyzer['anomaly_clusters']
            
            # ê±°ë¦¬ ê¸°ë°˜ ì´ìƒ íŒì •
            threshold_distance = np.mean([
                np.linalg.norm(center) for center in self.cluster_analyzer['cluster_centers']
            ])
            
            distance_anomaly = distance > threshold_distance * 1.5
            
            return {
                'is_anomaly': is_anomaly_cluster or distance_anomaly,
                'confidence': min(1.0, distance / threshold_distance),
                'cluster_label': cluster_label,
                'distance': distance
            }
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'is_anomaly': False, 'confidence': 0.0}
    
    def _integrate_adaptive_predictions(self, phase1_result: Dict, 
                                      adaptive_anomaly: Dict, 
                                      online_anomaly: Dict, 
                                      cluster_anomaly: Dict,
                                      features: Dict[str, float]) -> Dict:
        """ì ì‘í˜• ì˜ˆì¸¡ ê²°ê³¼ í†µí•©"""
        
        # ê° ì‹œìŠ¤í…œì˜ ì´ìƒ ì—¬ë¶€
        phase1_detected = phase1_result['is_anomaly']
        adaptive_detected = adaptive_anomaly['is_anomaly']
        online_detected = online_anomaly['is_anomaly']
        cluster_detected = cluster_anomaly['is_anomaly']
        
        # ê°€ì¤‘ íˆ¬í‘œ (ì ì‘í˜• ì‹œìŠ¤í…œì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        weights = {
            'phase1': 0.3,  # ê¸°ë³¸ ì‹œìŠ¤í…œ
            'adaptive': 0.4,  # ì ì‘í˜• ì„ê³„ê°’
            'online': 0.2,   # ì˜¨ë¼ì¸ í•™ìŠµ
            'cluster': 0.1   # í´ëŸ¬ìŠ¤í„° ë¶„ì„
        }
        
        # ê°€ì¤‘ í‰ê·  ì‹ ë¢°ë„
        weighted_confidence = (
            weights['phase1'] * phase1_result['confidence'] +
            weights['adaptive'] * adaptive_anomaly['confidence'] +
            weights['online'] * online_anomaly['confidence'] +
            weights['cluster'] * cluster_anomaly['confidence']
        )
        
        # ë‹¤ìˆ˜ê²° + ì‹ ë¢°ë„ ê¸°ë°˜ ìµœì¢… íŒì •
        votes = [phase1_detected, adaptive_detected, online_detected, cluster_detected]
        majority_vote = sum(votes) >= 2
        
        # ì‹ ë¢°ë„ê°€ ì„ê³„ê°’ ì´ìƒì´ë©´ ìµœì¢… íŒì •
        final_anomaly = majority_vote and weighted_confidence >= self.adaptation_params['confidence_threshold']
        
        # ì´ìƒ ìœ í˜• ë¶„ë¥˜ (ì ì‘í˜•)
        anomaly_type = self._classify_adaptive_anomaly_type(
            features, phase1_result, adaptive_anomaly, online_anomaly, cluster_anomaly
        )
        
        # ë©”ì‹œì§€ ìƒì„±
        message = self._generate_adaptive_message(
            final_anomaly, weighted_confidence, anomaly_type, votes
        )
        
        return {
            'is_anomaly': final_anomaly,
            'confidence': weighted_confidence,
            'message': message,
            'anomaly_type': anomaly_type,
            'individual_predictions': {
                'phase1': {
                    'prediction': phase1_detected,
                    'confidence': phase1_result['confidence'],
                    'anomaly_type': phase1_result.get('anomaly_type', 'unknown')
                },
                'adaptive': {
                    'prediction': adaptive_detected,
                    'confidence': adaptive_anomaly['confidence'],
                    'anomaly_features': adaptive_anomaly.get('anomaly_features', [])
                },
                'online': {
                    'prediction': online_detected,
                    'confidence': online_anomaly['confidence'],
                    'isolation_score': online_anomaly.get('isolation_score', 0.0)
                },
                'cluster': {
                    'prediction': cluster_detected,
                    'confidence': cluster_anomaly['confidence'],
                    'cluster_label': cluster_anomaly.get('cluster_label', -1)
                }
            },
            'voting_result': {
                'votes': votes,
                'majority_vote': majority_vote,
                'weights': weights
            }
        }
    
    def _classify_adaptive_anomaly_type(self, features: Dict[str, float], 
                                       phase1_result: Dict, 
                                       adaptive_anomaly: Dict, 
                                       online_anomaly: Dict, 
                                       cluster_anomaly: Dict) -> str:
        """ì ì‘í˜• ì´ìƒ ìœ í˜• ë¶„ë¥˜"""
        
        # Phase 1 ê²°ê³¼ê°€ ê°€ì¥ ì‹ ë¢°í•  ë§Œí•¨
        if phase1_result.get('anomaly_type') != 'normal':
            return phase1_result['anomaly_type']
        
        # ì ì‘í˜• ì„ê³„ê°’ ê¸°ë°˜ ë¶„ë¥˜
        anomaly_features = adaptive_anomaly.get('anomaly_features', [])
        if 'spectral_centroid' in anomaly_features and 'zero_crossing_rate' in anomaly_features:
            return 'bearing_wear'
        elif 'rms_energy' in anomaly_features:
            return 'compressor_abnormal'
        elif 'low_freq_ratio' in anomaly_features:
            return 'refrigerant_leak'
        
        # ì˜¨ë¼ì¸ í•™ìŠµ ê¸°ë°˜ ë¶„ë¥˜
        if online_anomaly.get('isolation_score', 0) < -1.0:
            return 'statistical_anomaly'
        
        # í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê¸°ë°˜ ë¶„ë¥˜
        if cluster_anomaly.get('cluster_label', -1) in self.cluster_analyzer['anomaly_clusters']:
            return 'cluster_anomaly'
        
        return 'general_anomaly'
    
    def _generate_adaptive_message(self, is_anomaly: bool, confidence: float, 
                                 anomaly_type: str, votes: List[bool]) -> str:
        """ì ì‘í˜• ë©”ì‹œì§€ ìƒì„±"""
        if not is_anomaly:
            return "ì •ìƒ ì‘ë™ ì¤‘ (ì ì‘í˜• ì‹œìŠ¤í…œ)"
        
        vote_count = sum(votes)
        confidence_level = "ë†’ìŒ" if confidence > 0.8 else "ë³´í†µ" if confidence > 0.6 else "ë‚®ìŒ"
        
        messages = {
            'bearing_wear': f'ë² ì–´ë§ ë§ˆëª¨ ì˜ì‹¬ ({vote_count}/4 ì‹œìŠ¤í…œ ì¼ì¹˜, ì‹ ë¢°ë„: {confidence_level})',
            'compressor_abnormal': f'ì••ì¶•ê¸° ì´ìƒ ì˜ì‹¬ ({vote_count}/4 ì‹œìŠ¤í…œ ì¼ì¹˜, ì‹ ë¢°ë„: {confidence_level})',
            'refrigerant_leak': f'ëƒ‰ë§¤ ëˆ„ì¶œ ì˜ì‹¬ ({vote_count}/4 ì‹œìŠ¤í…œ ì¼ì¹˜, ì‹ ë¢°ë„: {confidence_level})',
            'statistical_anomaly': f'í†µê³„ì  ì´ìƒ ê°ì§€ ({vote_count}/4 ì‹œìŠ¤í…œ ì¼ì¹˜, ì‹ ë¢°ë„: {confidence_level})',
            'cluster_anomaly': f'í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ì´ìƒ ê°ì§€ ({vote_count}/4 ì‹œìŠ¤í…œ ì¼ì¹˜, ì‹ ë¢°ë„: {confidence_level})',
            'general_anomaly': f'ì´ìƒ ì†ŒìŒ ê°ì§€ ({vote_count}/4 ì‹œìŠ¤í…œ ì¼ì¹˜, ì‹ ë¢°ë„: {confidence_level})'
        }
        
        return messages.get(anomaly_type, f'ì´ìƒ ê°ì§€ ({vote_count}/4 ì‹œìŠ¤í…œ ì¼ì¹˜, ì‹ ë¢°ë„: {confidence_level})')
    
    def _update_data_buffer(self, features: Dict[str, float], is_anomaly: bool, ground_truth: Optional[bool]):
        """ë°ì´í„° ë²„í¼ ì—…ë°ì´íŠ¸"""
        data_point = {
            'timestamp': datetime.now(),
            'features': features,
            'is_anomaly': is_anomaly,
            'ground_truth': ground_truth
        }
        
        self.data_buffer.append(data_point)
    
    def _perform_adaptive_update(self):
        """ì ì‘í˜• ì—…ë°ì´íŠ¸ ìˆ˜í–‰"""
        if len(self.data_buffer) < 50:  # ìµœì†Œ ë°ì´í„° í•„ìš”
            return
        
        print("ğŸ”„ ì ì‘í˜• ì—…ë°ì´íŠ¸ ìˆ˜í–‰ ì¤‘...")
        
        try:
            # 1. ì ì‘í˜• ì„ê³„ê°’ ì—…ë°ì´íŠ¸
            self._update_adaptive_thresholds()
            
            # 2. ì˜¨ë¼ì¸ í•™ìŠµ ì—…ë°ì´íŠ¸
            self._update_online_learning()
            
            # 3. í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì—…ë°ì´íŠ¸
            self._update_cluster_analysis()
            
            # 4. ì ì‘ í†µê³„ ì—…ë°ì´íŠ¸
            self.adaptation_stats['total_adaptations'] += 1
            self.adaptation_stats['last_adaptation'] = datetime.now().isoformat()
            
            print("âœ… ì ì‘í˜• ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ì ì‘í˜• ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def _update_adaptive_thresholds(self):
        """ì ì‘í˜• ì„ê³„ê°’ ì—…ë°ì´íŠ¸"""
        # ìµœê·¼ ë°ì´í„°ë§Œ ì‚¬ìš©
        recent_data = list(self.data_buffer)[-500:]  # ìµœê·¼ 500ê°œ ìƒ˜í”Œ
        
        for feature_name in self.feature_names:
            values = [d['features'][feature_name] for d in recent_data if feature_name in d['features']]
            
            if len(values) > 10:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì—…ë°ì´íŠ¸
                # í†µê³„ ê³„ì‚°
                mean_val = statistics.mean(values)
                std_val = statistics.stdev(values) if len(values) > 1 else 0
                
                # ë°±ë¶„ìœ„ìˆ˜
                percentiles = {
                    'p5': np.percentile(values, 5),
                    'p25': np.percentile(values, 25),
                    'p75': np.percentile(values, 75),
                    'p95': np.percentile(values, 95)
                }
                
                # ì ì‘í˜• ì„ê³„ê°’ ê³„ì‚°
                sensitivity = self.adaptation_params['sensitivity']
                lower_threshold = percentiles['p5'] * (1 - sensitivity)
                upper_threshold = percentiles['p95'] * (1 + sensitivity)
                
                # ì—…ë°ì´íŠ¸
                self.adaptive_thresholds[feature_name].update({
                    'lower': lower_threshold,
                    'upper': upper_threshold,
                    'mean': mean_val,
                    'std': std_val,
                    'percentiles': percentiles,
                    'update_count': self.adaptive_thresholds[feature_name]['update_count'] + 1,
                    'last_update': datetime.now().isoformat()
                })
        
        self.adaptation_stats['threshold_updates'] += 1
    
    def _update_online_learning(self):
        """ì˜¨ë¼ì¸ í•™ìŠµ ì—…ë°ì´íŠ¸"""
        # ì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš©
        normal_data = [d for d in self.data_buffer if not d['is_anomaly']]
        
        if len(normal_data) < 20:  # ìµœì†Œ ì •ìƒ ë°ì´í„° í•„ìš”
            return
        
        try:
            # íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
            feature_vectors = []
            for data_point in normal_data[-100:]:  # ìµœê·¼ 100ê°œë§Œ ì‚¬ìš©
                feature_vector = np.array(list(data_point['features'].values()))
                feature_vectors.append(feature_vector)
            
            X = np.array(feature_vectors)
            
            # ì •ê·œí™”
            X_scaled = self.online_learner['scaler'].fit_transform(X)
            
            # PCA
            n_components = min(8, X_scaled.shape[1])
            self.online_learner['pca'] = PCA(n_components=n_components, random_state=42)
            X_pca = self.online_learner['pca'].fit_transform(X_scaled)
            
            # Isolation Forest ì—…ë°ì´íŠ¸
            contamination = min(0.1, max(0.01, len([d for d in self.data_buffer if d['is_anomaly']]) / len(self.data_buffer)))
            self.online_learner['isolation_forest'] = IsolationForest(
                contamination=contamination,
                random_state=42,
                n_estimators=100
            )
            self.online_learner['isolation_forest'].fit(X_pca)
            
            self.online_learner['sample_count'] = len(feature_vectors)
            self.online_learner['last_update'] = datetime.now().isoformat()
            
            self.adaptation_stats['model_updates'] += 1
            
        except Exception as e:
            print(f"âŒ ì˜¨ë¼ì¸ í•™ìŠµ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def _update_cluster_analysis(self):
        """í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì—…ë°ì´íŠ¸"""
        if len(self.data_buffer) < 50:  # ìµœì†Œ ë°ì´í„° í•„ìš”
            return
        
        try:
            # íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
            feature_vectors = []
            for data_point in self.data_buffer:
                feature_vector = np.array(list(data_point['features'].values()))
                feature_vectors.append(feature_vector)
            
            X = np.array(feature_vectors)
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§
            n_clusters = min(5, len(X) // 10)  # ë°ì´í„° í¬ê¸°ì— ë”°ë¼ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì¡°ì •
            if n_clusters < 2:
                return
            
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(X)
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ì´ìƒ ë¹„ìœ¨ ê³„ì‚°
            cluster_anomaly_ratios = {}
            for cluster_id in range(n_clusters):
                cluster_data = [self.data_buffer[i] for i in range(len(cluster_labels)) if cluster_labels[i] == cluster_id]
                if cluster_data:
                    anomaly_count = sum(1 for d in cluster_data if d['is_anomaly'])
                    cluster_anomaly_ratios[cluster_id] = anomaly_count / len(cluster_data)
            
            # ì´ìƒ í´ëŸ¬ìŠ¤í„° ì‹ë³„ (ì´ìƒ ë¹„ìœ¨ì´ 0.3 ì´ìƒ)
            anomaly_clusters = {
                cluster_id for cluster_id, ratio in cluster_anomaly_ratios.items() 
                if ratio >= 0.3
            }
            normal_clusters = {
                cluster_id for cluster_id, ratio in cluster_anomaly_ratios.items() 
                if ratio < 0.3
            }
            
            # ì—…ë°ì´íŠ¸
            self.cluster_analyzer.update({
                'kmeans': kmeans,
                'cluster_centers': kmeans.cluster_centers_,
                'cluster_labels': cluster_labels,
                'anomaly_clusters': anomaly_clusters,
                'normal_clusters': normal_clusters,
                'cluster_stats': cluster_anomaly_ratios,
                'last_analysis': datetime.now().isoformat()
            })
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def _update_performance_metrics(self, result: Dict, ground_truth: Optional[bool], processing_time: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.performance_metrics['total_detections'] += 1
        
        if result['is_anomaly']:
            self.performance_metrics['anomaly_count'] += 1
        
        # ì •í™•ë„ ê³„ì‚°
        if ground_truth is not None:
            predicted = result['is_anomaly']
            if predicted and not ground_truth:
                self.performance_metrics['false_positives'] += 1
            elif not predicted and ground_truth:
                self.performance_metrics['false_negatives'] += 1
            
            # ì •í™•ë„ ì—…ë°ì´íŠ¸
            total = self.performance_metrics['total_detections']
            correct = total - (self.performance_metrics['false_positives'] + 
                             self.performance_metrics['false_negatives'])
            self.performance_metrics['accuracy'] = correct / total if total > 0 else 0.0
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.performance_metrics['total_detections']
        current_avg = self.performance_metrics['average_processing_time']
        self.performance_metrics['average_processing_time'] = (
            (current_avg * (total - 1) + processing_time) / total
        )
        
        self.performance_metrics['last_update'] = datetime.now().isoformat()
    
    def get_adaptive_stats(self) -> Dict:
        """ì ì‘í˜• í†µê³„ ë°˜í™˜"""
        stats = self.performance_metrics.copy()
        
        if stats['total_detections'] > 0:
            stats['anomaly_rate'] = stats['anomaly_count'] / stats['total_detections']
        else:
            stats['anomaly_rate'] = 0.0
        
        # ì ì‘í˜• í†µê³„ ì¶”ê°€
        stats.update({
            'phase': 'Phase 2',
            'adaptation_stats': self.adaptation_stats,
            'data_buffer_size': len(self.data_buffer),
            'adaptation_params': self.adaptation_params,
            'adaptive_thresholds_count': len(self.adaptive_thresholds),
            'online_learner_ready': self.online_learner['isolation_forest'] is not None,
            'cluster_analyzer_ready': self.cluster_analyzer['kmeans'] is not None
        })
        
        return stats
    
    def save_adaptive_system(self, filepath: str = None):
        """ì ì‘í˜• ì‹œìŠ¤í…œ ì €ì¥"""
        if filepath is None:
            filepath = os.path.join(self.model_save_path, "phase2_adaptive_system.pkl")
        
        # Phase 1 ì‹œìŠ¤í…œ ì €ì¥
        phase1_path = filepath.replace('phase2_adaptive_system.pkl', 'phase1_detector.pkl')
        if self.phase1_detector:
            self.phase1_detector.save_model(phase1_path)
        
        # ì ì‘í˜• ì‹œìŠ¤í…œ ë°ì´í„° ì €ì¥
        adaptive_data = {
            'adaptive_thresholds': self.adaptive_thresholds,
            'online_learner': {
                'scaler': self.online_learner['scaler'],
                'pca': self.online_learner['pca'],
                'isolation_forest': self.online_learner['isolation_forest'],
                'normal_stats': self.online_learner['normal_stats'],
                'anomaly_stats': self.online_learner['anomaly_stats'],
                'sample_count': self.online_learner['sample_count'],
                'last_update': self.online_learner['last_update']
            },
            'cluster_analyzer': self.cluster_analyzer,
            'adaptation_stats': self.adaptation_stats,
            'adaptation_params': self.adaptation_params,
            'performance_metrics': self.performance_metrics,
            'window_size': self.window_size,
            'sample_rate': self.sample_rate,
            'adaptation_interval': self.adaptation_interval,
            'phase': 'Phase 2'
        }
        
        joblib.dump(adaptive_data, filepath)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'phase2_adaptive_system',
            'phase': 'Phase 2',
            'created_at': datetime.now().isoformat(),
            'adaptation_stats': self.adaptation_stats,
            'performance_metrics': self.performance_metrics,
            'window_size': self.window_size,
            'sample_rate': self.sample_rate,
            'adaptation_interval': self.adaptation_interval
        }
        
        metadata_file = filepath.replace('.pkl', '_metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Phase 2 ì ì‘í˜• ì‹œìŠ¤í…œ ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_file}")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # Phase 2 ì ì‘í˜• ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    adaptive_system = Phase2AdaptiveSystem()
    
    print("ğŸ”„ Phase 2: ì ì‘í˜• ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print("í™˜ê²½ ë³€í™”ì— ìë™ìœ¼ë¡œ ì ì‘í•˜ëŠ” AI ì‹œìŠ¤í…œ")
    print("ë‹¤ì¤‘ ê²€ì¦ ë° ì‹¤ì‹œê°„ í•™ìŠµ")
    print("=" * 60)
    
    print("Phase 2 ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    print("Phase 1 ì‹œìŠ¤í…œê³¼ ì—°ë™ í›„ ì ì‘í˜• ì´ìƒ íƒì§€ ê°€ëŠ¥")
    print("ì˜ˆìƒ ì •í™•ë„: 85-90%")
    print("ì˜ˆìƒ ì²˜ë¦¬ ì†ë„: 50-100ms")
