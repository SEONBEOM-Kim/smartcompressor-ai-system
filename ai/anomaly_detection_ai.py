#!/usr/bin/env python3
"""
ëƒ‰ë™ê³  24ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ê¸°ë°˜ ì´ìƒ íƒì§€ AI
ì •ìƒ ë°ì´í„°ê°€ ëŒ€ë¶€ë¶„ì¸ ìƒí™©ì— ìµœì í™”ëœ ì‹œìŠ¤í…œ
"""

import numpy as np
import librosa
import time
from scipy import signal
from scipy.signal import butter, filtfilt
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import joblib
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json

class RefrigeratorAnomalyDetector:
    def __init__(self, model_save_path: str = "data/models/"):
        """
        ëƒ‰ë™ê³  ì´ìƒ íƒì§€ AI ì´ˆê¸°í™”
        
        Args:
            model_save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        self.model_save_path = model_save_path
        os.makedirs(model_save_path, exist_ok=True)
        
        # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì„¤ì •
        self.sample_rate = 16000
        self.n_fft = 1024
        self.hop_length = 512
        self.window_size = 5.0  # 5ì´ˆ ìœˆë„ìš°
        
        # ì´ìƒ íƒì§€ ëª¨ë¸ë“¤
        self.isolation_forest = None
        self.scaler = StandardScaler()
        self.pca = None
        self.is_trained = False
        
        # ì •ìƒ ë°ì´í„° í†µê³„ (ì ì‘í˜• ì„ê³„ê°’ìš©)
        self.normal_stats = {
            'rms_energy': {'mean': 0, 'std': 0, 'percentiles': {}},
            'spectral_centroid': {'mean': 0, 'std': 0, 'percentiles': {}},
            'zero_crossing_rate': {'mean': 0, 'std': 0, 'percentiles': {}},
            'spectral_rolloff': {'mean': 0, 'std': 0, 'percentiles': {}},
            'mfcc_means': {'mean': 0, 'std': 0},
            'mfcc_stds': {'mean': 0, 'std': 0}
        }
        
        # ì ì‘í˜• ì„ê³„ê°’ ì„¤ì •
        self.adaptive_thresholds = {
            'rms_energy': {'lower': 0, 'upper': 0},
            'spectral_centroid': {'lower': 0, 'upper': 0},
            'zero_crossing_rate': {'lower': 0, 'upper': 0},
            'spectral_rolloff': {'lower': 0, 'upper': 0},
            'isolation_score': {'threshold': 0}
        }
        
        # ëª¨ë‹ˆí„°ë§ íˆìŠ¤í† ë¦¬ (ìµœê·¼ 24ì‹œê°„)
        self.monitoring_history = []
        self.max_history_hours = 24
        
        print("ğŸ§  ëƒ‰ë™ê³  ì´ìƒ íƒì§€ AI ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“Š ëª¨ë‹ˆí„°ë§ ìœˆë„ìš°: {self.window_size}ì´ˆ")
        print(f"ğŸ“ˆ ì ì‘í˜• ì„ê³„ê°’ ì‹œìŠ¤í…œ í™œì„±í™”")
    
    def extract_comprehensive_features(self, audio_data: np.ndarray, sr: int) -> Dict[str, float]:
        """ì¢…í•©ì ì¸ íŠ¹ì§• ì¶”ì¶œ (ì •ìƒ ë°ì´í„° íŒ¨í„´ í•™ìŠµìš©)"""
        try:
            # ê¸°ë³¸ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
            if sr != self.sample_rate:
                audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=self.sample_rate)
            
            # ë…¸ì´ì¦ˆ í•„í„°ë§
            nyquist = self.sample_rate / 2
            low_cutoff = 50 / nyquist
            high_cutoff = 4000 / nyquist
            b, a = butter(4, [low_cutoff, high_cutoff], btype='band')
            filtered_audio = filtfilt(b, a, audio_data)
            
            # 1. ì—ë„ˆì§€ ê¸°ë°˜ íŠ¹ì§•
            rms_energy = np.sqrt(np.mean(filtered_audio ** 2))
            energy_entropy = self._calculate_energy_entropy(filtered_audio)
            
            # 2. ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì§•
            stft = librosa.stft(filtered_audio, n_fft=self.n_fft, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            frequencies = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.n_fft)
            
            # ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬, ë¡¤ì˜¤í”„, ëŒ€ì—­í­
            spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=filtered_audio, sr=self.sample_rate))
            spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=filtered_audio, sr=self.sample_rate))
            spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=filtered_audio, sr=self.sample_rate))
            
            # 3. Zero Crossing Rate (ë‚ ì¹´ë¡œìš´ ì†ŒìŒ ê°ì§€)
            zcr = np.mean(librosa.feature.zero_crossing_rate(filtered_audio, hop_length=self.hop_length))
            
            # 4. MFCC íŠ¹ì§• (ìŒì„± ì¸ì‹ì—ì„œ íš¨ê³¼ì )
            mfccs = librosa.feature.mfcc(y=filtered_audio, sr=self.sample_rate, n_mfcc=13)
            mfcc_means = np.mean(mfccs, axis=1)
            mfcc_stds = np.std(mfccs, axis=1)
            
            # 5. ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì§•
            spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=filtered_audio, sr=self.sample_rate))
            spectral_flatness = np.mean(librosa.feature.spectral_flatness(y=filtered_audio))
            
            # 6. ì£¼íŒŒìˆ˜ ëŒ€ì—­ë³„ ì—ë„ˆì§€ ë¹„ìœ¨
            low_freq_energy = np.sum(magnitude[(frequencies >= 50) & (frequencies <= 500), :])
            mid_freq_energy = np.sum(magnitude[(frequencies >= 500) & (frequencies <= 2000), :])
            high_freq_energy = np.sum(magnitude[(frequencies >= 2000) & (frequencies <= 4000), :])
            total_energy = np.sum(magnitude)
            
            if total_energy > 0:
                low_freq_ratio = low_freq_energy / total_energy
                mid_freq_ratio = mid_freq_energy / total_energy
                high_freq_ratio = high_freq_energy / total_energy
            else:
                low_freq_ratio = mid_freq_ratio = high_freq_ratio = 0
            
            # 7. ì‹œê°„ì  íŠ¹ì§• (ë³€ë™ì„±)
            rms_temporal = librosa.feature.rms(y=filtered_audio, hop_length=self.hop_length)[0]
            temporal_std = np.std(rms_temporal)
            temporal_mean = np.mean(rms_temporal)
            
            features = {
                # ì—ë„ˆì§€ íŠ¹ì§•
                'rms_energy': float(rms_energy),
                'energy_entropy': float(energy_entropy),
                'temporal_std': float(temporal_std),
                'temporal_mean': float(temporal_mean),
                
                # ì£¼íŒŒìˆ˜ íŠ¹ì§•
                'spectral_centroid': float(spectral_centroid),
                'spectral_rolloff': float(spectral_rolloff),
                'spectral_bandwidth': float(spectral_bandwidth),
                'spectral_contrast': float(spectral_contrast),
                'spectral_flatness': float(spectral_flatness),
                
                # Zero Crossing Rate
                'zero_crossing_rate': float(zcr),
                
                # ì£¼íŒŒìˆ˜ ëŒ€ì—­ ë¹„ìœ¨
                'low_freq_ratio': float(low_freq_ratio),
                'mid_freq_ratio': float(mid_freq_ratio),
                'high_freq_ratio': float(high_freq_ratio),
                
                # MFCC í†µê³„
                'mfcc_1_mean': float(mfcc_means[0]),
                'mfcc_2_mean': float(mfcc_means[1]),
                'mfcc_3_mean': float(mfcc_means[2]),
                'mfcc_1_std': float(mfcc_stds[0]),
                'mfcc_2_std': float(mfcc_stds[1]),
                'mfcc_3_std': float(mfcc_stds[2])
            }
            
            return features
            
        except Exception as e:
            print(f"âŒ íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return self._get_default_features()
    
    def _calculate_energy_entropy(self, audio_data: np.ndarray) -> float:
        """ì—ë„ˆì§€ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        try:
            # í”„ë ˆì„ ë‹¨ìœ„ë¡œ ì—ë„ˆì§€ ê³„ì‚°
            frame_length = 1024
            hop_length = 512
            frames = librosa.util.frame(audio_data, frame_length=frame_length, hop_length=hop_length)
            frame_energies = np.sum(frames ** 2, axis=0)
            
            # ì •ê·œí™”
            if np.sum(frame_energies) > 0:
                frame_energies = frame_energies / np.sum(frame_energies)
                # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
                frame_energies = frame_energies + 1e-10
                entropy = -np.sum(frame_energies * np.log2(frame_energies))
            else:
                entropy = 0
            
            return float(entropy)
        except:
            return 0.0
    
    def _get_default_features(self) -> Dict[str, float]:
        """ê¸°ë³¸ íŠ¹ì§•ê°’ ë°˜í™˜ (ì˜¤ë¥˜ ì‹œ)"""
        return {
            'rms_energy': 0.0, 'energy_entropy': 0.0, 'temporal_std': 0.0, 'temporal_mean': 0.0,
            'spectral_centroid': 0.0, 'spectral_rolloff': 0.0, 'spectral_bandwidth': 0.0,
            'spectral_contrast': 0.0, 'spectral_flatness': 0.0, 'zero_crossing_rate': 0.0,
            'low_freq_ratio': 0.0, 'mid_freq_ratio': 0.0, 'high_freq_ratio': 0.0,
            'mfcc_1_mean': 0.0, 'mfcc_2_mean': 0.0, 'mfcc_3_mean': 0.0,
            'mfcc_1_std': 0.0, 'mfcc_2_std': 0.0, 'mfcc_3_std': 0.0
        }
    
    def train_on_normal_data(self, normal_audio_files: List[str], 
                           validation_split: float = 0.2) -> Dict:
        """
        ì •ìƒ ë°ì´í„°ë¡œë§Œ í›ˆë ¨ (ì´ìƒ íƒì§€ ëª¨ë¸)
        
        Args:
            normal_audio_files: ì •ìƒ ìƒíƒœ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            validation_split: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
        """
        print("ğŸ¯ ì •ìƒ ë°ì´í„° ê¸°ë°˜ ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        print(f"ğŸ“ ì •ìƒ ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜: {len(normal_audio_files)}")
        
        # ì •ìƒ ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ
        normal_features = []
        for i, file_path in enumerate(normal_audio_files):
            try:
                if i % 10 == 0:
                    print(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(normal_audio_files)}")
                
                audio_data, sr = librosa.load(file_path, sr=None)
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ì„
                chunk_samples = int(self.window_size * sr)
                for start in range(0, len(audio_data), chunk_samples):
                    chunk = audio_data[start:start + chunk_samples]
                    if len(chunk) >= chunk_samples:
                        features = self.extract_comprehensive_features(chunk, sr)
                        normal_features.append(list(features.values()))
                        
            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                continue
        
        if len(normal_features) < 10:
            raise ValueError("ì¶©ë¶„í•œ ì •ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 10ê°œ ìƒ˜í”Œ í•„ìš”")
        
        # íŠ¹ì§• ì •ê·œí™”
        X_normal = np.array(normal_features)
        X_scaled = self.scaler.fit_transform(X_normal)
        
        # PCAë¡œ ì°¨ì› ì¶•ì†Œ (ë…¸ì´ì¦ˆ ê°ì†Œ)
        self.pca = PCA(n_components=min(10, X_scaled.shape[1]), random_state=42)
        X_pca = self.pca.fit_transform(X_scaled)
        
        # Isolation Forest í›ˆë ¨ (ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ)
        self.isolation_forest = IsolationForest(
            contamination=0.05,  # 5%ë¥¼ ì´ìƒìœ¼ë¡œ ê°„ì£¼
            random_state=42,
            n_estimators=100
        )
        
        # ì •ìƒ ë°ì´í„°ë¡œ í›ˆë ¨
        self.isolation_forest.fit(X_pca)
        
        # ì •ìƒ ë°ì´í„° í†µê³„ ê³„ì‚° (ì ì‘í˜• ì„ê³„ê°’ìš©)
        self._calculate_normal_statistics(X_normal)
        
        # ì ì‘í˜• ì„ê³„ê°’ ì„¤ì •
        self._set_adaptive_thresholds(X_pca)
        
        self.is_trained = True
        
        # í›ˆë ¨ ê²°ê³¼ ì €ì¥
        training_result = {
            'total_samples': len(normal_features),
            'feature_dimensions': X_normal.shape[1],
            'pca_components': self.pca.n_components_,
            'explained_variance_ratio': float(np.sum(self.pca.explained_variance_ratio_)),
            'isolation_forest_score': float(np.mean(self.isolation_forest.score_samples(X_pca))),
            'training_completed': True
        }
        
        print("âœ… ì •ìƒ ë°ì´í„° ê¸°ë°˜ í›ˆë ¨ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {training_result['total_samples']}")
        print(f"ğŸ“ˆ PCA ì„¤ëª… ë¶„ì‚°: {training_result['explained_variance_ratio']:.3f}")
        
        return training_result
    
    def _calculate_normal_statistics(self, X_normal: np.ndarray):
        """ì •ìƒ ë°ì´í„° í†µê³„ ê³„ì‚°"""
        feature_names = list(self._get_default_features().keys())
        
        for i, feature_name in enumerate(feature_names):
            if i < X_normal.shape[1]:
                values = X_normal[:, i]
                self.normal_stats[feature_name] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'percentiles': {
                        'p5': float(np.percentile(values, 5)),
                        'p25': float(np.percentile(values, 25)),
                        'p75': float(np.percentile(values, 75)),
                        'p95': float(np.percentile(values, 95))
                    }
                }
    
    def _set_adaptive_thresholds(self, X_pca: np.ndarray):
        """ì ì‘í˜• ì„ê³„ê°’ ì„¤ì •"""
        # Isolation Forest ì ìˆ˜ ê¸°ë°˜ ì„ê³„ê°’
        scores = self.isolation_forest.score_samples(X_pca)
        self.adaptive_thresholds['isolation_score']['threshold'] = float(np.percentile(scores, 5))
        
        # ê°œë³„ íŠ¹ì§• ê¸°ë°˜ ì„ê³„ê°’ (3ì‹œê·¸ë§ˆ ê·œì¹™)
        for feature_name, stats in self.normal_stats.items():
            if 'percentiles' in stats:
                lower = stats['percentiles']['p5']  # í•˜ìœ„ 5%
                upper = stats['percentiles']['p95']  # ìƒìœ„ 5%
                self.adaptive_thresholds[feature_name] = {
                    'lower': lower,
                    'upper': upper
                }
    
    def detect_anomaly(self, audio_data: np.ndarray, sr: int) -> Dict:
        """
        ì‹¤ì‹œê°„ ì´ìƒ íƒì§€
        
        Args:
            audio_data: ì˜¤ë””ì˜¤ ë°ì´í„°
            sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            
        Returns:
            ì´ìƒ íƒì§€ ê²°ê³¼
        """
        if not self.is_trained:
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': 'ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                'anomaly_type': 'unknown',
                'features': {},
                'processing_time_ms': 0
            }
        
        try:
            start_time = time.time()
            
            # íŠ¹ì§• ì¶”ì¶œ
            features = self.extract_comprehensive_features(audio_data, sr)
            feature_vector = np.array(list(features.values())).reshape(1, -1)
            
            # ì •ê·œí™” ë° PCA ë³€í™˜
            X_scaled = self.scaler.transform(feature_vector)
            X_pca = self.pca.transform(X_scaled)
            
            # Isolation Forest ì ìˆ˜ ê³„ì‚°
            isolation_score = self.isolation_forest.score_samples(X_pca)[0]
            is_anomaly = isolation_score < self.adaptive_thresholds['isolation_score']['threshold']
            
            # ê°œë³„ íŠ¹ì§• ê¸°ë°˜ ì´ìƒ íƒì§€
            feature_anomalies = self._check_individual_features(features)
            
            # ì´ìƒ ìœ í˜• ë¶„ë¥˜
            anomaly_type = self._classify_anomaly_type(features, feature_anomalies)
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence(isolation_score, feature_anomalies)
            
            # ìµœì¢… íŒì •
            final_anomaly = is_anomaly or len(feature_anomalies) > 2
            
            processing_time = (time.time() - start_time) * 1000
            
            # ëª¨ë‹ˆí„°ë§ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self._update_monitoring_history({
                'timestamp': datetime.now(),
                'is_anomaly': final_anomaly,
                'confidence': confidence,
                'anomaly_type': anomaly_type,
                'isolation_score': float(isolation_score),
                'feature_anomalies': feature_anomalies
            })
            
            result = {
                'is_anomaly': final_anomaly,
                'confidence': confidence,
                'message': self._get_anomaly_message(anomaly_type, confidence),
                'anomaly_type': anomaly_type,
                'features': features,
                'isolation_score': float(isolation_score),
                'feature_anomalies': feature_anomalies,
                'processing_time_ms': processing_time,
                'adaptive_thresholds': self.adaptive_thresholds
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ ì´ìƒ íƒì§€ ì˜¤ë¥˜: {e}")
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': f'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
                'anomaly_type': 'error',
                'features': {},
                'processing_time_ms': 0
            }
    
    def _check_individual_features(self, features: Dict[str, float]) -> List[str]:
        """ê°œë³„ íŠ¹ì§• ê¸°ë°˜ ì´ìƒ íƒì§€"""
        anomalies = []
        
        for feature_name, value in features.items():
            if feature_name in self.adaptive_thresholds:
                threshold = self.adaptive_thresholds[feature_name]
                if value < threshold['lower'] or value > threshold['upper']:
                    anomalies.append(feature_name)
        
        return anomalies
    
    def _classify_anomaly_type(self, features: Dict[str, float], 
                              feature_anomalies: List[str]) -> str:
        """ì´ìƒ ìœ í˜• ë¶„ë¥˜"""
        if not feature_anomalies:
            return 'normal'
        
        # ë² ì–´ë§ ë§ˆëª¨ (ê³ ì£¼íŒŒ, ZCR ë†’ìŒ)
        if 'spectral_centroid' in feature_anomalies and 'zero_crossing_rate' in feature_anomalies:
            return 'bearing_wear'
        
        # ì••ì¶•ê¸° ì´ìƒ (ì—ë„ˆì§€ íŒ¨í„´ ë³€í™”)
        if 'rms_energy' in feature_anomalies and 'temporal_std' in feature_anomalies:
            return 'compressor_abnormal'
        
        # ëƒ‰ë§¤ ëˆ„ì¶œ (ì €ì£¼íŒŒ ì—ë„ˆì§€ ì¦ê°€)
        if 'low_freq_ratio' in feature_anomalies:
            return 'refrigerant_leak'
        
        # ì¼ë°˜ì ì¸ ì´ìƒ
        return 'general_anomaly'
    
    def _calculate_confidence(self, isolation_score: float, 
                            feature_anomalies: List[str]) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        # Isolation Forest ì ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„
        score_confidence = min(1.0, max(0.0, abs(isolation_score) / 2.0))
        
        # íŠ¹ì§• ê¸°ë°˜ ì‹ ë¢°ë„
        feature_confidence = min(1.0, len(feature_anomalies) / 5.0)
        
        # ê°€ì¤‘ í‰ê· 
        confidence = 0.7 * score_confidence + 0.3 * feature_confidence
        return float(confidence)
    
    def _get_anomaly_message(self, anomaly_type: str, confidence: float) -> str:
        """ì´ìƒ ë©”ì‹œì§€ ìƒì„±"""
        messages = {
            'normal': 'ì •ìƒ ì‘ë™ ì¤‘',
            'bearing_wear': f'ë² ì–´ë§ ë§ˆëª¨ ì˜ì‹¬ (ì‹ ë¢°ë„: {confidence:.1%})',
            'compressor_abnormal': f'ì••ì¶•ê¸° ì´ìƒ ì˜ì‹¬ (ì‹ ë¢°ë„: {confidence:.1%})',
            'refrigerant_leak': f'ëƒ‰ë§¤ ëˆ„ì¶œ ì˜ì‹¬ (ì‹ ë¢°ë„: {confidence:.1%})',
            'general_anomaly': f'ì´ìƒ ì†ŒìŒ ê°ì§€ (ì‹ ë¢°ë„: {confidence:.1%})',
            'error': 'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ'
        }
        return messages.get(anomaly_type, 'ì•Œ ìˆ˜ ì—†ëŠ” ì´ìƒ')
    
    def _update_monitoring_history(self, result: Dict):
        """ëª¨ë‹ˆí„°ë§ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸"""
        self.monitoring_history.append(result)
        
        # 24ì‹œê°„ ì´ì „ ë°ì´í„° ì œê±°
        cutoff_time = datetime.now() - timedelta(hours=self.max_history_hours)
        self.monitoring_history = [
            r for r in self.monitoring_history 
            if r['timestamp'] > cutoff_time
        ]
    
    def get_monitoring_summary(self) -> Dict:
        """ëª¨ë‹ˆí„°ë§ ìš”ì•½ ì •ë³´"""
        if not self.monitoring_history:
            return {'total_samples': 0, 'anomaly_rate': 0.0}
        
        total_samples = len(self.monitoring_history)
        anomaly_count = sum(1 for r in self.monitoring_history if r['is_anomaly'])
        anomaly_rate = anomaly_count / total_samples if total_samples > 0 else 0.0
        
        # ìµœê·¼ ì´ìƒ ìœ í˜•ë³„ í†µê³„
        recent_anomalies = [r for r in self.monitoring_history if r['is_anomaly']]
        anomaly_types = {}
        for anomaly in recent_anomalies:
            anomaly_type = anomaly['anomaly_type']
            anomaly_types[anomaly_type] = anomaly_types.get(anomaly_type, 0) + 1
        
        return {
            'total_samples': total_samples,
            'anomaly_count': anomaly_count,
            'anomaly_rate': float(anomaly_rate),
            'anomaly_types': anomaly_types,
            'monitoring_duration_hours': self.max_history_hours,
            'last_update': self.monitoring_history[-1]['timestamp'].isoformat() if self.monitoring_history else None
        }
    
    def save_model(self, filepath: str = None):
        """ëª¨ë¸ ì €ì¥"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if filepath is None:
            filepath = os.path.join(self.model_save_path, "anomaly_detector.pkl")
        
        # ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ì €ì¥
        model_data = {
            'isolation_forest': self.isolation_forest,
            'scaler': self.scaler,
            'pca': self.pca,
            'normal_stats': self.normal_stats,
            'adaptive_thresholds': self.adaptive_thresholds,
            'is_trained': self.is_trained,
            'sample_rate': self.sample_rate,
            'window_size': self.window_size
        }
        
        joblib.dump(model_data, filepath)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'refrigerator_anomaly_detector',
            'created_at': datetime.now().isoformat(),
            'feature_count': len(self._get_default_features()),
            'pca_components': self.pca.n_components_ if self.pca else 0,
            'contamination_rate': 0.05
        }
        
        metadata_file = filepath.replace('.pkl', '_metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ì´ìƒ íƒì§€ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_file}")
    
    def load_model(self, filepath: str = None):
        """ëª¨ë¸ ë¡œë“œ"""
        if filepath is None:
            filepath = os.path.join(self.model_save_path, "anomaly_detector.pkl")
        
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        
        # ëª¨ë¸ ë¡œë“œ
        model_data = joblib.load(filepath)
        
        self.isolation_forest = model_data['isolation_forest']
        self.scaler = model_data['scaler']
        self.pca = model_data['pca']
        self.normal_stats = model_data['normal_stats']
        self.adaptive_thresholds = model_data['adaptive_thresholds']
        self.is_trained = model_data['is_trained']
        self.sample_rate = model_data['sample_rate']
        self.window_size = model_data['window_size']
        
        print(f"âœ… ì´ìƒ íƒì§€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # ì´ìƒ íƒì§€ AI ì´ˆê¸°í™”
    detector = RefrigeratorAnomalyDetector()
    
    print("ğŸ§  ëƒ‰ë™ê³  ì´ìƒ íƒì§€ AI ì‹œìŠ¤í…œ")
    print("=" * 50)
    print("ì •ìƒ ë°ì´í„°ë¡œ í›ˆë ¨ í›„ ì´ìƒ íƒì§€ ìˆ˜í–‰")
    print("24ì‹œê°„ ëª¨ë‹ˆí„°ë§ì— ìµœì í™”ëœ ì‹œìŠ¤í…œ")
    print("=" * 50)
