#!/usr/bin/env python3
"""
ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ
24ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ ì§€ì† ê°œì„ 
"""

import numpy as np
import joblib
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json
from collections import deque
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import threading
import time

class OnlineLearningSystem:
    def __init__(self, 
                 model_save_path: str = "data/models/",
                 learning_rate: float = 0.01,
                 memory_size: int = 10000,
                 update_frequency: int = 100):
        """
        ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            model_save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            learning_rate: í•™ìŠµë¥ 
            memory_size: ë©”ëª¨ë¦¬ì— ìœ ì§€í•  ìƒ˜í”Œ ìˆ˜
            update_frequency: ëª¨ë¸ ì—…ë°ì´íŠ¸ ì£¼ê¸° (ìƒ˜í”Œ ìˆ˜)
        """
        self.model_save_path = model_save_path
        os.makedirs(model_save_path, exist_ok=True)
        
        self.learning_rate = learning_rate
        self.memory_size = memory_size
        self.update_frequency = update_frequency
        
        # í•™ìŠµ ë°ì´í„° ë²„í¼
        self.feature_buffer = deque(maxlen=memory_size)
        self.label_buffer = deque(maxlen=memory_size)
        self.timestamp_buffer = deque(maxlen=memory_size)
        
        # ëª¨ë¸ë“¤
        self.isolation_forest = None
        self.scaler = StandardScaler()
        self.pca = None
        
        # í†µê³„ ì •ë³´
        self.normal_stats = {}
        self.anomaly_stats = {}
        
        # í•™ìŠµ ìƒíƒœ
        self.is_learning = False
        self.total_samples = 0
        self.last_update = None
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„ ìœ„í•œ ë½
        self.lock = threading.Lock()
        
        print(f"ğŸ§  ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"ğŸ“š í•™ìŠµë¥ : {learning_rate}")
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ í¬ê¸°: {memory_size}")
        print(f"ğŸ”„ ì—…ë°ì´íŠ¸ ì£¼ê¸°: {update_frequency}ìƒ˜í”Œ")
    
    def add_sample(self, features: Dict[str, float], 
                  is_anomaly: bool, 
                  confidence: float = 1.0,
                  timestamp: datetime = None):
        """
        ìƒˆë¡œìš´ ìƒ˜í”Œ ì¶”ê°€ ë° ì˜¨ë¼ì¸ í•™ìŠµ
        
        Args:
            features: ì¶”ì¶œëœ íŠ¹ì§•
            is_anomaly: ì´ìƒ ì—¬ë¶€
            confidence: ì‹ ë¢°ë„ (0.0-1.0)
            timestamp: ì‹œê°„
        """
        if timestamp is None:
            timestamp = datetime.now()
        
        with self.lock:
            # íŠ¹ì§• ë²¡í„°ë¡œ ë³€í™˜
            feature_vector = np.array(list(features.values()))
            
            # ë²„í¼ì— ì¶”ê°€
            self.feature_buffer.append(feature_vector)
            self.label_buffer.append(is_anomaly)
            self.timestamp_buffer.append(timestamp)
            
            self.total_samples += 1
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_statistics(feature_vector, is_anomaly, confidence)
            
            # ì£¼ê¸°ì  ëª¨ë¸ ì—…ë°ì´íŠ¸
            if self.total_samples % self.update_frequency == 0:
                self._trigger_model_update()
    
    def _update_statistics(self, features: np.ndarray, 
                          is_anomaly: bool, confidence: float):
        """í†µê³„ ì •ë³´ ì—…ë°ì´íŠ¸"""
        feature_names = list(self._get_default_features().keys())
        
        for i, feature_name in enumerate(feature_names):
            if i < len(features):
                value = features[i]
                
                if is_anomaly:
                    if feature_name not in self.anomaly_stats:
                        self.anomaly_stats[feature_name] = {
                            'values': [],
                            'count': 0,
                            'mean': 0,
                            'std': 0
                        }
                    
                    stats = self.anomaly_stats[feature_name]
                    stats['values'].append(value)
                    stats['count'] += 1
                    
                    # ì´ë™ í‰ê· ìœ¼ë¡œ í†µê³„ ì—…ë°ì´íŠ¸
                    if stats['count'] == 1:
                        stats['mean'] = value
                        stats['std'] = 0
                    else:
                        old_mean = stats['mean']
                        stats['mean'] = old_mean + (value - old_mean) / stats['count']
                        stats['std'] = np.sqrt(
                            (stats['std']**2 * (stats['count']-1) + (value - stats['mean'])**2) / stats['count']
                        )
                    
                    # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
                    if len(stats['values']) > 1000:
                        stats['values'] = stats['values'][-1000:]
                else:
                    if feature_name not in self.normal_stats:
                        self.normal_stats[feature_name] = {
                            'values': [],
                            'count': 0,
                            'mean': 0,
                            'std': 0
                        }
                    
                    stats = self.normal_stats[feature_name]
                    stats['values'].append(value)
                    stats['count'] += 1
                    
                    # ì´ë™ í‰ê· ìœ¼ë¡œ í†µê³„ ì—…ë°ì´íŠ¸
                    if stats['count'] == 1:
                        stats['mean'] = value
                        stats['std'] = 0
                    else:
                        old_mean = stats['mean']
                        stats['mean'] = old_mean + (value - old_mean) / stats['count']
                        stats['std'] = np.sqrt(
                            (stats['std']**2 * (stats['count']-1) + (value - stats['mean'])**2) / stats['count']
                        )
                    
                    # ìµœê·¼ 5000ê°œë§Œ ìœ ì§€
                    if len(stats['values']) > 5000:
                        stats['values'] = stats['values'][-5000:]
    
    def _trigger_model_update(self):
        """ëª¨ë¸ ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±°"""
        if len(self.feature_buffer) < 50:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            return
        
        print(f"ğŸ”„ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì‹œì‘ (ìƒ˜í”Œ ìˆ˜: {len(self.feature_buffer)})")
        
        try:
            # ë°ì´í„° ì¤€ë¹„
            X = np.array(list(self.feature_buffer))
            y = np.array(list(self.label_buffer))
            
            # ì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš© (ì´ìƒ íƒì§€ ëª¨ë¸)
            normal_mask = ~y
            X_normal = X[normal_mask]
            
            if len(X_normal) < 10:
                print("âŒ ì •ìƒ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ëª¨ë¸ ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€")
                return
            
            # ì •ê·œí™”
            X_scaled = self.scaler.fit_transform(X_normal)
            
            # PCA ì°¨ì› ì¶•ì†Œ
            n_components = min(10, X_scaled.shape[1])
            self.pca = PCA(n_components=n_components, random_state=42)
            X_pca = self.pca.fit_transform(X_scaled)
            
            # Isolation Forest ì—…ë°ì´íŠ¸
            contamination = min(0.1, max(0.01, np.sum(y) / len(y)))
            self.isolation_forest = IsolationForest(
                contamination=contamination,
                random_state=42,
                n_estimators=100
            )
            self.isolation_forest.fit(X_pca)
            
            self.last_update = datetime.now()
            print(f"âœ… ëª¨ë¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì˜¤ì—¼ë¥ : {contamination:.3f})")
            
            # ëª¨ë¸ ì €ì¥
            self._save_model()
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def predict(self, features: Dict[str, float]) -> Dict:
        """
        ì´ìƒ íƒì§€ ì˜ˆì¸¡
        
        Args:
            features: í™•ì¸í•  íŠ¹ì§•
            
        Returns:
            ì˜ˆì¸¡ ê²°ê³¼
        """
        if self.isolation_forest is None:
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': 'ëª¨ë¸ì´ ì•„ì§ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                'anomaly_score': 0.0
            }
        
        try:
            # íŠ¹ì§• ë²¡í„° ë³€í™˜
            feature_vector = np.array(list(features.values())).reshape(1, -1)
            
            # ì •ê·œí™” ë° PCA ë³€í™˜
            X_scaled = self.scaler.transform(feature_vector)
            X_pca = self.pca.transform(X_scaled)
            
            # ì´ìƒ íƒì§€
            anomaly_score = self.isolation_forest.score_samples(X_pca)[0]
            is_anomaly = anomaly_score < 0  # ìŒìˆ˜ë©´ ì´ìƒ
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = min(1.0, max(0.0, abs(anomaly_score)))
            
            # í†µê³„ ê¸°ë°˜ ì¶”ê°€ ê²€ì¦
            statistical_anomaly = self._check_statistical_anomaly(features)
            
            # ìµœì¢… íŒì •
            final_anomaly = is_anomaly or statistical_anomaly
            
            message = self._get_anomaly_message(final_anomaly, confidence, statistical_anomaly)
            
            return {
                'is_anomaly': final_anomaly,
                'confidence': confidence,
                'message': message,
                'anomaly_score': float(anomaly_score),
                'statistical_anomaly': statistical_anomaly,
                'model_samples': len(self.feature_buffer),
                'last_update': self.last_update.isoformat() if self.last_update else None
            }
            
        except Exception as e:
            print(f"âŒ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': f'ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
                'anomaly_score': 0.0
            }
    
    def _check_statistical_anomaly(self, features: Dict[str, float]) -> bool:
        """í†µê³„ ê¸°ë°˜ ì´ìƒ ê²€ì‚¬"""
        if not self.normal_stats:
            return False
        
        anomaly_count = 0
        total_features = 0
        
        for feature_name, value in features.items():
            if feature_name in self.normal_stats:
                stats = self.normal_stats[feature_name]
                if stats['count'] > 0 and stats['std'] > 0:
                    # Z-score ê³„ì‚°
                    z_score = abs((value - stats['mean']) / stats['std'])
                    if z_score > 3:  # 3ì‹œê·¸ë§ˆ ê·œì¹™
                        anomaly_count += 1
                    total_features += 1
        
        # 30% ì´ìƒì˜ íŠ¹ì§•ì´ ì´ìƒì´ë©´ ì „ì²´ë¥¼ ì´ìƒìœ¼ë¡œ íŒì •
        return total_features > 0 and (anomaly_count / total_features) > 0.3
    
    def _get_anomaly_message(self, is_anomaly: bool, confidence: float, 
                           statistical_anomaly: bool) -> str:
        """ì´ìƒ ë©”ì‹œì§€ ìƒì„±"""
        if not is_anomaly:
            return "ì •ìƒ ì‘ë™ ì¤‘"
        
        if statistical_anomaly:
            return f"í†µê³„ì  ì´ìƒ ê°ì§€ (ì‹ ë¢°ë„: {confidence:.1%})"
        else:
            return f"ëª¨ë¸ ê¸°ë°˜ ì´ìƒ ê°ì§€ (ì‹ ë¢°ë„: {confidence:.1%})"
    
    def _get_default_features(self) -> Dict[str, float]:
        """ê¸°ë³¸ íŠ¹ì§•ê°’ ë°˜í™˜"""
        return {
            'rms_energy': 0.0, 'energy_entropy': 0.0, 'temporal_std': 0.0,
            'spectral_centroid': 0.0, 'spectral_rolloff': 0.0, 'spectral_bandwidth': 0.0,
            'zero_crossing_rate': 0.0, 'low_freq_ratio': 0.0, 'mid_freq_ratio': 0.0,
            'high_freq_ratio': 0.0, 'mfcc_1_mean': 0.0, 'mfcc_2_mean': 0.0
        }
    
    def get_learning_statistics(self) -> Dict:
        """í•™ìŠµ í†µê³„ ì •ë³´"""
        with self.lock:
            normal_count = sum(1 for label in self.label_buffer if not label)
            anomaly_count = sum(1 for label in self.label_buffer if label)
            
            return {
                'total_samples': self.total_samples,
                'buffer_size': len(self.feature_buffer),
                'normal_samples': normal_count,
                'anomaly_samples': anomaly_count,
                'anomaly_rate': anomaly_count / len(self.label_buffer) if self.label_buffer else 0,
                'last_update': self.last_update.isoformat() if self.last_update else None,
                'learning_rate': self.learning_rate,
                'update_frequency': self.update_frequency
            }
    
    def get_feature_statistics(self) -> Dict:
        """íŠ¹ì§•ë³„ í†µê³„ ì •ë³´"""
        with self.lock:
            return {
                'normal_stats': self.normal_stats.copy(),
                'anomaly_stats': self.anomaly_stats.copy()
            }
    
    def _save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        try:
            model_data = {
                'isolation_forest': self.isolation_forest,
                'scaler': self.scaler,
                'pca': self.pca,
                'normal_stats': self.normal_stats,
                'anomaly_stats': self.anomaly_stats,
                'total_samples': self.total_samples,
                'last_update': self.last_update.isoformat() if self.last_update else None,
                'learning_rate': self.learning_rate
            }
            
            filepath = os.path.join(self.model_save_path, "online_learning_model.pkl")
            joblib.dump(model_data, filepath)
            
            print(f"ğŸ’¾ ì˜¨ë¼ì¸ í•™ìŠµ ëª¨ë¸ ì €ì¥: {filepath}")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def load_model(self, filepath: str = None):
        """ëª¨ë¸ ë¡œë“œ"""
        if filepath is None:
            filepath = os.path.join(self.model_save_path, "online_learning_model.pkl")
        
        if not os.path.exists(filepath):
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
            return
        
        try:
            with self.lock:
                model_data = joblib.load(filepath)
                
                self.isolation_forest = model_data['isolation_forest']
                self.scaler = model_data['scaler']
                self.pca = model_data['pca']
                self.normal_stats = model_data['normal_stats']
                self.anomaly_stats = model_data['anomaly_stats']
                self.total_samples = model_data['total_samples']
                self.learning_rate = model_data['learning_rate']
                
                if model_data['last_update']:
                    self.last_update = datetime.fromisoformat(model_data['last_update'])
                
                print(f"ğŸ“‚ ì˜¨ë¼ì¸ í•™ìŠµ ëª¨ë¸ ë¡œë“œ: {filepath}")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    def reset_learning(self):
        """í•™ìŠµ ìƒíƒœ ì´ˆê¸°í™”"""
        with self.lock:
            self.feature_buffer.clear()
            self.label_buffer.clear()
            self.timestamp_buffer.clear()
            self.normal_stats.clear()
            self.anomaly_stats.clear()
            self.total_samples = 0
            self.last_update = None
            
            print("ğŸ”„ í•™ìŠµ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def adjust_learning_rate(self, new_rate: float):
        """í•™ìŠµë¥  ì¡°ì •"""
        if 0.001 <= new_rate <= 1.0:
            self.learning_rate = new_rate
            print(f"ğŸ“š í•™ìŠµë¥  ì¡°ì •: {new_rate}")
        else:
            print("âŒ í•™ìŠµë¥ ì€ 0.001-1.0 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤.")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    online_learner = OnlineLearningSystem(
        learning_rate=0.01,
        memory_size=5000,
        update_frequency=50
    )
    
    print("ğŸ§  ì˜¨ë¼ì¸ í•™ìŠµ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 40)
    
    # ê°€ìƒì˜ í•™ìŠµ ë°ì´í„°
    for i in range(200):
        features = {
            'rms_energy': np.random.normal(0.5, 0.1),
            'spectral_centroid': np.random.normal(1500, 200),
            'zero_crossing_rate': np.random.normal(0.1, 0.02)
        }
        
        # 90% ì •ìƒ, 10% ì´ìƒ
        is_anomaly = np.random.random() < 0.1
        online_learner.add_sample(features, is_anomaly)
    
    # í•™ìŠµ í†µê³„ í™•ì¸
    stats = online_learner.get_learning_statistics()
    print(f"ğŸ“Š í•™ìŠµ í†µê³„: {stats}")
    
    # ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    test_features = {
        'rms_energy': 1.5,  # ì´ìƒì ìœ¼ë¡œ ë†’ìŒ
        'spectral_centroid': 800,  # ì´ìƒì ìœ¼ë¡œ ë‚®ìŒ
        'zero_crossing_rate': 0.3  # ì´ìƒì ìœ¼ë¡œ ë†’ìŒ
    }
    
    prediction = online_learner.predict(test_features)
    print(f"ğŸ” ì˜ˆì¸¡ ê²°ê³¼: {prediction}")
