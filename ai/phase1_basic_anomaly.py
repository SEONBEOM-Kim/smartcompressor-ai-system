#!/usr/bin/env python3
"""
Phase 1: ê¸°ë³¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ
ì •ìƒ ë°ì´í„° ê¸°ë°˜ ì´ìƒ íƒì§€ AI êµ¬í˜„
"""

import numpy as np
import librosa
import time
import os
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from collections import deque
import threading
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import joblib

class Phase1BasicAnomalyDetector:
    def __init__(self, 
                 model_save_path: str = "data/models/phase1/",
                 window_size: float = 5.0,
                 sample_rate: int = 16000):
        """
        Phase 1 ê¸°ë³¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            model_save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            window_size: ë¶„ì„ ìœˆë„ìš° í¬ê¸° (ì´ˆ)
            sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
        """
        self.model_save_path = model_save_path
        os.makedirs(model_save_path, exist_ok=True)
        
        self.window_size = window_size
        self.sample_rate = sample_rate
        self.n_fft = 1024
        self.hop_length = 512
        
        # AI ëª¨ë¸ë“¤
        self.isolation_forest = None
        self.scaler = StandardScaler()
        self.pca = None
        self.is_trained = False
        
        # íŠ¹ì§• ì´ë¦„ ì •ì˜
        self.feature_names = [
            'rms_energy', 'spectral_centroid', 'spectral_rolloff', 
            'zero_crossing_rate', 'spectral_bandwidth', 'spectral_contrast',
            'low_freq_ratio', 'mid_freq_ratio', 'high_freq_ratio',
            'mfcc_1_mean', 'mfcc_2_mean', 'mfcc_3_mean'
        ]
        
        # í†µê³„ ì •ë³´
        self.normal_stats = {}
        self.performance_metrics = {
            'total_detections': 0,
            'anomaly_count': 0,
            'false_positives': 0,
            'false_negatives': 0,
            'average_processing_time': 0.0,
            'accuracy': 0.0,
            'last_update': None
        }
        
        # íƒì§€ íˆìŠ¤í† ë¦¬
        self.detection_history = deque(maxlen=1000)
        
        print("ğŸš€ Phase 1: ê¸°ë³¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"â±ï¸ ìœˆë„ìš° í¬ê¸°: {window_size}ì´ˆ")
        print(f"ğŸµ ìƒ˜í”Œë§ ë ˆì´íŠ¸: {sample_rate}Hz")
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_save_path}")
    
    def extract_enhanced_features(self, audio_data: np.ndarray, sr: int) -> Dict[str, float]:
        """í–¥ìƒëœ íŠ¹ì§• ì¶”ì¶œ (Phase 1 ìµœì í™”)"""
        try:
            # ë¦¬ìƒ˜í”Œë§
            if sr != self.sample_rate:
                audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=self.sample_rate)
            
            # ë…¸ì´ì¦ˆ í•„í„°ë§ (í–¥ìƒëœ í•„í„°)
            nyquist = self.sample_rate / 2
            low_cutoff = 50 / nyquist
            high_cutoff = 4000 / nyquist
            b, a = librosa.filters.butter(4, [low_cutoff, high_cutoff], btype='band')
            filtered_audio = librosa.filters.filtfilt(b, a, audio_data)
            
            # 1. ì—ë„ˆì§€ íŠ¹ì§• (í–¥ìƒëœ)
            rms_energy = np.sqrt(np.mean(filtered_audio ** 2))
            energy_entropy = self._calculate_energy_entropy(filtered_audio)
            
            # 2. ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì§•
            stft = librosa.stft(filtered_audio, n_fft=self.n_fft, hop_length=self.hop_length)
            magnitude = np.abs(stft)
            frequencies = librosa.fft_frequencies(sr=self.sample_rate, n_fft=self.n_fft)
            
            # ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì§•
            spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=filtered_audio, sr=self.sample_rate))
            spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=filtered_audio, sr=self.sample_rate))
            spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=filtered_audio, sr=self.sample_rate))
            spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=filtered_audio, sr=self.sample_rate))
            
            # Zero Crossing Rate
            zcr = np.mean(librosa.feature.zero_crossing_rate(filtered_audio, hop_length=self.hop_length))
            
            # 3. ì£¼íŒŒìˆ˜ ëŒ€ì—­ë³„ ì—ë„ˆì§€ ë¹„ìœ¨ (í–¥ìƒëœ)
            low_freq_energy = np.sum(magnitude[(frequencies >= 50) & (frequencies <= 500), :])
            mid_freq_energy = np.sum(magnitude[(frequencies >= 500) & (frequencies <= 2000), :])
            high_freq_energy = np.sum(magnitude[(frequencies >= 2000) & (frequencies <= 4000), :])
            total_energy = np.sum(magnitude)
            
            if total_energy > 0:
                low_freq_ratio = low_freq_energy / total_energy
                mid_freq_ratio = mid_freq_energy / total_energy
                high_freq_ratio = high_freq_energy / total_energy
            else:
                low_freq_ratio = mid_freq_ratio = high_freq_ratio = 0
            
            # 4. MFCC íŠ¹ì§• (ì²« 3ê°œë§Œ ì‚¬ìš©, ìµœì í™”)
            mfccs = librosa.feature.mfcc(y=filtered_audio, sr=self.sample_rate, n_mfcc=13)
            mfcc_means = np.mean(mfccs, axis=1)
            
            # 5. ì‹œê°„ì  íŠ¹ì§• (ìƒˆë¡œ ì¶”ê°€)
            rms_temporal = librosa.feature.rms(y=filtered_audio, hop_length=self.hop_length)[0]
            temporal_std = np.std(rms_temporal)
            temporal_mean = np.mean(rms_temporal)
            
            features = {
                'rms_energy': float(rms_energy),
                'energy_entropy': float(energy_entropy),
                'spectral_centroid': float(spectral_centroid),
                'spectral_rolloff': float(spectral_rolloff),
                'zero_crossing_rate': float(zcr),
                'spectral_bandwidth': float(spectral_bandwidth),
                'spectral_contrast': float(spectral_contrast),
                'low_freq_ratio': float(low_freq_ratio),
                'mid_freq_ratio': float(mid_freq_ratio),
                'high_freq_ratio': float(high_freq_ratio),
                'mfcc_1_mean': float(mfcc_means[0]),
                'mfcc_2_mean': float(mfcc_means[1]),
                'mfcc_3_mean': float(mfcc_means[2]),
                'temporal_std': float(temporal_std),
                'temporal_mean': float(temporal_mean)
            }
            
            return features
            
        except Exception as e:
            print(f"âŒ íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return {name: 0.0 for name in self.feature_names}
    
    def _calculate_energy_entropy(self, audio_data: np.ndarray) -> float:
        """ì—ë„ˆì§€ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°"""
        try:
            # í”„ë ˆì„ ë‹¨ìœ„ë¡œ ì—ë„ˆì§€ ê³„ì‚°
            frame_length = 1024
            hop_length = 512
            frames = librosa.util.frame(audio_data, frame_length=frame_length, hop_length=hop_length)
            frame_energies = np.sum(frames ** 2, axis=0)
            
            # ì •ê·œí™”
            if np.sum(frame_energies) > 0:
                frame_energies = frame_energies / np.sum(frame_energies)
                # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
                frame_energies = frame_energies + 1e-10
                entropy = -np.sum(frame_energies * np.log2(frame_energies))
            else:
                entropy = 0
            
            return float(entropy)
        except:
            return 0.0
    
    def train_on_normal_data(self, normal_audio_files: List[str], 
                           validation_split: float = 0.2) -> Dict:
        """
        ì •ìƒ ë°ì´í„°ë¡œ í›ˆë ¨ (Phase 1 ìµœì í™”)
        
        Args:
            normal_audio_files: ì •ìƒ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            validation_split: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            
        Returns:
            í›ˆë ¨ ê²°ê³¼
        """
        print("ğŸ¯ Phase 1: ì •ìƒ ë°ì´í„°ë¡œ ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        print(f"ğŸ“ ì •ìƒ ì˜¤ë””ì˜¤ íŒŒì¼ ìˆ˜: {len(normal_audio_files)}")
        
        # ì •ìƒ ë°ì´í„° íŠ¹ì§• ì¶”ì¶œ
        normal_features = []
        processed_files = 0
        start_time = time.time()
        
        for i, file_path in enumerate(normal_audio_files):
            try:
                if i % 10 == 0:
                    print(f"  ì²˜ë¦¬ ì¤‘: {i+1}/{len(normal_audio_files)}")
                
                audio_data, sr = librosa.load(file_path, sr=None)
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ì„
                chunk_samples = int(self.window_size * sr)
                for start in range(0, len(audio_data), chunk_samples):
                    chunk = audio_data[start:start + chunk_samples]
                    if len(chunk) >= chunk_samples:
                        features = self.extract_enhanced_features(chunk, sr)
                        normal_features.append(list(features.values()))
                        processed_files += 1
                        
            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}: {e}")
                continue
        
        if len(normal_features) < 10:
            raise ValueError("ì¶©ë¶„í•œ ì •ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ 10ê°œ ìƒ˜í”Œ í•„ìš”")
        
        print(f"âœ… ì²˜ë¦¬ëœ ìƒ˜í”Œ ìˆ˜: {len(normal_features)}")
        
        # íŠ¹ì§• ì •ê·œí™”
        X_normal = np.array(normal_features)
        X_scaled = self.scaler.fit_transform(X_normal)
        
        # PCAë¡œ ì°¨ì› ì¶•ì†Œ (ë…¸ì´ì¦ˆ ê°ì†Œ)
        n_components = min(10, X_scaled.shape[1])
        self.pca = PCA(n_components=n_components, random_state=42)
        X_pca = self.pca.fit_transform(X_scaled)
        
        # Isolation Forest í›ˆë ¨ (ìµœì í™”ëœ íŒŒë¼ë¯¸í„°)
        contamination = 0.05  # 5%ë¥¼ ì´ìƒìœ¼ë¡œ ê°„ì£¼
        self.isolation_forest = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=200,  # ë” ë§ì€ íŠ¸ë¦¬ë¡œ ì •í™•ë„ í–¥ìƒ
            max_samples='auto',
            max_features=1.0,
            n_jobs=-1  # CPU ë³‘ë ¬ ì²˜ë¦¬
        )
        
        # ì •ìƒ ë°ì´í„°ë¡œ í›ˆë ¨
        self.isolation_forest.fit(X_pca)
        
        # ì •ìƒ ë°ì´í„° í†µê³„ ì €ì¥
        self._calculate_normal_statistics(X_normal)
        
        self.is_trained = True
        training_time = time.time() - start_time
        
        # í›ˆë ¨ ê²°ê³¼
        result = {
            'total_samples': len(normal_features),
            'processed_files': processed_files,
            'feature_dimensions': X_normal.shape[1],
            'pca_components': n_components,
            'explained_variance_ratio': float(np.sum(self.pca.explained_variance_ratio_)),
            'contamination_rate': contamination,
            'training_time_seconds': training_time,
            'training_completed': True,
            'training_timestamp': datetime.now().isoformat()
        }
        
        print("âœ… Phase 1 ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {result['total_samples']}")
        print(f"ğŸ“ˆ PCA ì„¤ëª… ë¶„ì‚°: {result['explained_variance_ratio']:.3f}")
        print(f"â±ï¸ í›ˆë ¨ ì‹œê°„: {training_time:.2f}ì´ˆ")
        
        return result
    
    def _calculate_normal_statistics(self, X_normal: np.ndarray):
        """ì •ìƒ ë°ì´í„° í†µê³„ ê³„ì‚° (í–¥ìƒëœ)"""
        for i, feature_name in enumerate(self.feature_names):
            if i < X_normal.shape[1]:
                values = X_normal[:, i]
                self.normal_stats[feature_name] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'min': float(np.min(values)),
                    'max': float(np.max(values)),
                    'percentiles': {
                        'p1': float(np.percentile(values, 1)),
                        'p5': float(np.percentile(values, 5)),
                        'p25': float(np.percentile(values, 25)),
                        'p50': float(np.percentile(values, 50)),
                        'p75': float(np.percentile(values, 75)),
                        'p95': float(np.percentile(values, 95)),
                        'p99': float(np.percentile(values, 99))
                    }
                }
    
    def detect_anomaly(self, audio_data: np.ndarray, sr: int, 
                      ground_truth: Optional[bool] = None) -> Dict:
        """
        ì´ìƒ íƒì§€ ìˆ˜í–‰ (Phase 1 ìµœì í™”)
        
        Args:
            audio_data: ì˜¤ë””ì˜¤ ë°ì´í„°
            sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            ground_truth: ì‹¤ì œ ì´ìƒ ì—¬ë¶€ (ì„±ëŠ¥ í‰ê°€ìš©)
            
        Returns:
            ì´ìƒ íƒì§€ ê²°ê³¼
        """
        if not self.is_trained:
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': 'ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                'anomaly_type': 'model_not_trained',
                'processing_time_ms': 0
            }
        
        start_time = time.time()
        
        try:
            # íŠ¹ì§• ì¶”ì¶œ
            features = self.extract_enhanced_features(audio_data, sr)
            feature_vector = np.array(list(features.values())).reshape(1, -1)
            
            # ì •ê·œí™” ë° PCA ë³€í™˜
            X_scaled = self.scaler.transform(feature_vector)
            X_pca = self.pca.transform(X_scaled)
            
            # Isolation Forest ì ìˆ˜ ê³„ì‚°
            isolation_score = self.isolation_forest.score_samples(X_pca)[0]
            is_anomaly = isolation_score < 0  # ìŒìˆ˜ë©´ ì´ìƒ
            
            # í†µê³„ ê¸°ë°˜ ì¶”ê°€ ê²€ì¦ (í–¥ìƒëœ)
            statistical_anomaly = self._check_enhanced_statistical_anomaly(features)
            
            # ì´ìƒ ìœ í˜• ë¶„ë¥˜ (í–¥ìƒëœ)
            anomaly_type = self._classify_enhanced_anomaly_type(features, statistical_anomaly)
            
            # ì‹ ë¢°ë„ ê³„ì‚° (í–¥ìƒëœ)
            confidence = self._calculate_enhanced_confidence(isolation_score, statistical_anomaly, features)
            
            # ìµœì¢… íŒì • (ë‹¤ì¤‘ ê²€ì¦)
            final_anomaly = is_anomaly or statistical_anomaly
            
            processing_time = (time.time() - start_time) * 1000
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_performance_metrics(final_anomaly, ground_truth, processing_time)
            
            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.detection_history.append({
                'timestamp': datetime.now(),
                'is_anomaly': final_anomaly,
                'confidence': confidence,
                'anomaly_type': anomaly_type,
                'isolation_score': float(isolation_score),
                'statistical_anomaly': statistical_anomaly,
                'processing_time_ms': processing_time,
                'ground_truth': ground_truth
            })
            
            result = {
                'is_anomaly': final_anomaly,
                'confidence': confidence,
                'message': self._get_enhanced_anomaly_message(anomaly_type, confidence),
                'anomaly_type': anomaly_type,
                'features': features,
                'isolation_score': float(isolation_score),
                'statistical_anomaly': statistical_anomaly,
                'processing_time_ms': processing_time,
                'phase': 'Phase 1'
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ ì´ìƒ íƒì§€ ì˜¤ë¥˜: {e}")
            return {
                'is_anomaly': False,
                'confidence': 0.0,
                'message': f'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}',
                'anomaly_type': 'error',
                'processing_time_ms': (time.time() - start_time) * 1000
            }
    
    def _check_enhanced_statistical_anomaly(self, features: Dict[str, float]) -> bool:
        """í–¥ìƒëœ í†µê³„ ê¸°ë°˜ ì´ìƒ ê²€ì‚¬"""
        if not self.normal_stats:
            return False
        
        anomaly_count = 0
        total_features = 0
        anomaly_scores = []
        
        for feature_name, value in features.items():
            if feature_name in self.normal_stats:
                stats = self.normal_stats[feature_name]
                if stats['std'] > 0:
                    # Z-score ê³„ì‚°
                    z_score = abs((value - stats['mean']) / stats['std'])
                    anomaly_scores.append(z_score)
                    
                    # 3ì‹œê·¸ë§ˆ ê·œì¹™
                    if z_score > 3:
                        anomaly_count += 1
                    total_features += 1
        
        # ê°€ì¤‘ í‰ê·  ì´ìƒ ì ìˆ˜
        if anomaly_scores:
            avg_anomaly_score = np.mean(anomaly_scores)
            # 30% ì´ìƒì˜ íŠ¹ì§•ì´ ì´ìƒì´ê±°ë‚˜ í‰ê·  ì´ìƒ ì ìˆ˜ê°€ ë†’ìœ¼ë©´ ì´ìƒìœ¼ë¡œ íŒì •
            return (total_features > 0 and (anomaly_count / total_features) > 0.3) or avg_anomaly_score > 2.5
        
        return False
    
    def _classify_enhanced_anomaly_type(self, features: Dict[str, float], 
                                       statistical_anomaly: bool) -> str:
        """í–¥ìƒëœ ì´ìƒ ìœ í˜• ë¶„ë¥˜"""
        if not statistical_anomaly:
            return 'normal'
        
        # ë² ì–´ë§ ë§ˆëª¨ (ê³ ì£¼íŒŒ, ZCR ë†’ìŒ, ì—ë„ˆì§€ ì—”íŠ¸ë¡œí”¼ ë†’ìŒ)
        if (features.get('spectral_centroid', 0) > 2000 and 
            features.get('zero_crossing_rate', 0) > 0.2 and
            features.get('energy_entropy', 0) > 0.5):
            return 'bearing_wear'
        
        # ì••ì¶•ê¸° ì´ìƒ (ì—ë„ˆì§€ íŒ¨í„´ ë³€í™”, ì‹œê°„ì  ë³€ë™ì„±)
        if (features.get('rms_energy', 0) > 1.0 or 
            features.get('rms_energy', 0) < 0.01 or
            features.get('temporal_std', 0) > 0.5):
            return 'compressor_abnormal'
        
        # ëƒ‰ë§¤ ëˆ„ì¶œ (ì €ì£¼íŒŒ ì—ë„ˆì§€ ì¦ê°€, ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬ ë‚®ìŒ)
        if (features.get('low_freq_ratio', 0) > 0.8 and
            features.get('spectral_centroid', 0) < 1000):
            return 'refrigerant_leak'
        
        # ê³ ì£¼íŒŒ ì´ìƒ (ê³ ì£¼íŒŒ ì—ë„ˆì§€ ì¦ê°€)
        if features.get('high_freq_ratio', 0) > 0.6:
            return 'high_frequency_anomaly'
        
        # ì¼ë°˜ì ì¸ ì´ìƒ
        return 'general_anomaly'
    
    def _calculate_enhanced_confidence(self, isolation_score: float, 
                                     statistical_anomaly: bool, 
                                     features: Dict[str, float]) -> float:
        """í–¥ìƒëœ ì‹ ë¢°ë„ ê³„ì‚°"""
        # Isolation Forest ì ìˆ˜ ê¸°ë°˜ ì‹ ë¢°ë„
        score_confidence = min(1.0, max(0.0, abs(isolation_score) / 2.0))
        
        # í†µê³„ ê¸°ë°˜ ì‹ ë¢°ë„
        stat_confidence = 0.8 if statistical_anomaly else 0.2
        
        # íŠ¹ì§• ê¸°ë°˜ ì‹ ë¢°ë„ (ìƒˆë¡œ ì¶”ê°€)
        feature_confidence = 0.5
        if statistical_anomaly:
            # ì´ìƒ íŠ¹ì§•ì˜ ê°•ë„ì— ë”°ë¼ ì‹ ë¢°ë„ ì¡°ì •
            anomaly_strength = 0
            for feature_name, value in features.items():
                if feature_name in self.normal_stats:
                    stats = self.normal_stats[feature_name]
                    if stats['std'] > 0:
                        z_score = abs((value - stats['mean']) / stats['std'])
                        anomaly_strength += min(1.0, z_score / 3.0)
            
            feature_confidence = min(1.0, anomaly_strength / len(features))
        
        # ê°€ì¤‘ í‰ê·  (í–¥ìƒëœ)
        confidence = (0.4 * score_confidence + 
                     0.4 * stat_confidence + 
                     0.2 * feature_confidence)
        
        return float(confidence)
    
    def _get_enhanced_anomaly_message(self, anomaly_type: str, confidence: float) -> str:
        """í–¥ìƒëœ ì´ìƒ ë©”ì‹œì§€ ìƒì„±"""
        messages = {
            'normal': 'ì •ìƒ ì‘ë™ ì¤‘',
            'bearing_wear': f'ë² ì–´ë§ ë§ˆëª¨ ì˜ì‹¬ (ì‹ ë¢°ë„: {confidence:.1%})',
            'compressor_abnormal': f'ì••ì¶•ê¸° ì´ìƒ ì˜ì‹¬ (ì‹ ë¢°ë„: {confidence:.1%})',
            'refrigerant_leak': f'ëƒ‰ë§¤ ëˆ„ì¶œ ì˜ì‹¬ (ì‹ ë¢°ë„: {confidence:.1%})',
            'high_frequency_anomaly': f'ê³ ì£¼íŒŒ ì´ìƒ ê°ì§€ (ì‹ ë¢°ë„: {confidence:.1%})',
            'general_anomaly': f'ì´ìƒ ì†ŒìŒ ê°ì§€ (ì‹ ë¢°ë„: {confidence:.1%})',
            'model_not_trained': 'ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',
            'error': 'ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ'
        }
        return messages.get(anomaly_type, 'ì•Œ ìˆ˜ ì—†ëŠ” ì´ìƒ')
    
    def _update_performance_metrics(self, is_anomaly: bool, ground_truth: Optional[bool], 
                                   processing_time: float):
        """ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸ (í–¥ìƒëœ)"""
        self.performance_metrics['total_detections'] += 1
        
        if is_anomaly:
            self.performance_metrics['anomaly_count'] += 1
        
        # ì •í™•ë„ ê³„ì‚° (ground_truthê°€ ìˆëŠ” ê²½ìš°)
        if ground_truth is not None:
            predicted = is_anomaly
            if predicted and not ground_truth:
                self.performance_metrics['false_positives'] += 1
            elif not predicted and ground_truth:
                self.performance_metrics['false_negatives'] += 1
            
            # ì •í™•ë„ ì—…ë°ì´íŠ¸
            total = self.performance_metrics['total_detections']
            correct = total - (self.performance_metrics['false_positives'] + 
                             self.performance_metrics['false_negatives'])
            self.performance_metrics['accuracy'] = correct / total if total > 0 else 0.0
        
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        total = self.performance_metrics['total_detections']
        current_avg = self.performance_metrics['average_processing_time']
        self.performance_metrics['average_processing_time'] = (
            (current_avg * (total - 1) + processing_time) / total
        )
        
        self.performance_metrics['last_update'] = datetime.now().isoformat()
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜ (í–¥ìƒëœ)"""
        stats = self.performance_metrics.copy()
        
        if stats['total_detections'] > 0:
            stats['anomaly_rate'] = stats['anomaly_count'] / stats['total_detections']
        else:
            stats['anomaly_rate'] = 0.0
        
        # ì¶”ê°€ í†µê³„
        stats['phase'] = 'Phase 1'
        stats['model_trained'] = self.is_trained
        stats['feature_count'] = len(self.feature_names)
        
        return stats
    
    def get_detection_history(self, limit: int = 100) -> List[Dict]:
        """ìµœê·¼ íƒì§€ íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return list(self.detection_history)[-limit:]
    
    def save_model(self, filepath: str = None):
        """ëª¨ë¸ ì €ì¥ (Phase 1)"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if filepath is None:
            filepath = os.path.join(self.model_save_path, "phase1_anomaly_detector.pkl")
        
        # ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ì €ì¥
        model_data = {
            'isolation_forest': self.isolation_forest,
            'scaler': self.scaler,
            'pca': self.pca,
            'normal_stats': self.normal_stats,
            'feature_names': self.feature_names,
            'window_size': self.window_size,
            'sample_rate': self.sample_rate,
            'is_trained': self.is_trained,
            'performance_metrics': self.performance_metrics,
            'phase': 'Phase 1'
        }
        
        joblib.dump(model_data, filepath)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'phase1_anomaly_detector',
            'phase': 'Phase 1',
            'created_at': datetime.now().isoformat(),
            'feature_count': len(self.feature_names),
            'pca_components': self.pca.n_components_ if self.pca else 0,
            'contamination_rate': 0.05,
            'window_size': self.window_size,
            'sample_rate': self.sample_rate,
            'performance_metrics': self.performance_metrics
        }
        
        metadata_file = filepath.replace('.pkl', '_metadata.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Phase 1 ì´ìƒ íƒì§€ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_file}")
    
    def load_model(self, filepath: str = None):
        """ëª¨ë¸ ë¡œë“œ (Phase 1)"""
        if filepath is None:
            filepath = os.path.join(self.model_save_path, "phase1_anomaly_detector.pkl")
        
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filepath}")
        
        # ëª¨ë¸ ë¡œë“œ
        model_data = joblib.load(filepath)
        
        self.isolation_forest = model_data['isolation_forest']
        self.scaler = model_data['scaler']
        self.pca = model_data['pca']
        self.normal_stats = model_data['normal_stats']
        self.feature_names = model_data['feature_names']
        self.window_size = model_data['window_size']
        self.sample_rate = model_data['sample_rate']
        self.is_trained = model_data['is_trained']
        self.performance_metrics = model_data.get('performance_metrics', {})
        
        print(f"âœ… Phase 1 ì´ìƒ íƒì§€ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # Phase 1 ê¸°ë³¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    detector = Phase1BasicAnomalyDetector()
    
    print("ğŸš€ Phase 1: ê¸°ë³¸ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print("ì •ìƒ ë°ì´í„° ê¸°ë°˜ ì´ìƒ íƒì§€ AI")
    print("í–¥ìƒëœ íŠ¹ì§• ì¶”ì¶œ ë° ë‹¤ì¤‘ ê²€ì¦")
    print("=" * 60)
    
    # ê°€ìƒì˜ ì •ìƒ ë°ì´í„°ë¡œ í›ˆë ¨ (ì‹¤ì œë¡œëŠ” íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)
    # normal_files = ["normal1.wav", "normal2.wav", ...]
    # training_result = detector.train_on_normal_data(normal_files)
    
    print("Phase 1 ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    print("ì •ìƒ ë°ì´í„°ë¡œ í›ˆë ¨ í›„ ì‹¤ì‹œê°„ ì´ìƒ íƒì§€ ê°€ëŠ¥")
    print("ì˜ˆìƒ ì •í™•ë„: 80-85%")
    print("ì˜ˆìƒ ì²˜ë¦¬ ì†ë„: 80-150ms")
