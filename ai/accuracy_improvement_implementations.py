#!/usr/bin/env python3
"""
ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ ë‹¤ì–‘í•œ êµ¬í˜„ ë°©ë²•ë“¤
ì‚¬ìš©ìê°€ ì„ íƒí•œ ë°©ë²•ì— ë”°ë¼ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ì½”ë“œ ì˜ˆì‹œë“¤
"""

import numpy as np
import librosa
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import joblib
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class AccuracyImprovementMethods:
    """ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì„ êµ¬í˜„í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.feature_selector = None
        self.models = {}
        self.feature_importance = {}
        
        print("ğŸš€ ì •í™•ë„ í–¥ìƒ ë°©ë²•ë“¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # ===== 1. ë°ì´í„° ì¦ê°• (Data Augmentation) =====
    
    def augment_audio_data(self, audio_data: np.ndarray, sr: int, 
                          augmentation_factor: int = 3) -> List[np.ndarray]:
        """
        ì˜¤ë””ì˜¤ ë°ì´í„° ì¦ê°•
        
        Args:
            audio_data: ì›ë³¸ ì˜¤ë””ì˜¤ ë°ì´í„°
            sr: ìƒ˜í”Œë§ ë ˆì´íŠ¸
            augmentation_factor: ì¦ê°• ë°°ìˆ˜
            
        Returns:
            ì¦ê°•ëœ ì˜¤ë””ì˜¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        augmented_data = [audio_data]  # ì›ë³¸ í¬í•¨
        
        for i in range(augmentation_factor):
            # 1. ë…¸ì´ì¦ˆ ì¶”ê°€
            noise_factor = np.random.uniform(0.001, 0.01)
            noise = np.random.normal(0, noise_factor, len(audio_data))
            noisy_audio = audio_data + noise
            augmented_data.append(noisy_audio)
            
            # 2. ì‹œê°„ ë³€í˜• (ì†ë„ ì¡°ì ˆ)
            speed_factor = np.random.uniform(0.8, 1.2)
            if speed_factor != 1.0:
                try:
                    time_stretched = librosa.effects.time_stretch(audio_data, rate=speed_factor)
                    # ê¸¸ì´ ë§ì¶”ê¸°
                    if len(time_stretched) > len(audio_data):
                        time_stretched = time_stretched[:len(audio_data)]
                    else:
                        time_stretched = np.pad(time_stretched, (0, len(audio_data) - len(time_stretched)))
                    augmented_data.append(time_stretched)
                except:
                    pass
            
            # 3. ì£¼íŒŒìˆ˜ ë³€í˜• (í”¼ì¹˜ ì¡°ì ˆ)
            pitch_shift = np.random.randint(-2, 3)
            if pitch_shift != 0:
                try:
                    pitch_shifted = librosa.effects.pitch_shift(audio_data, sr=sr, n_steps=pitch_shift)
                    augmented_data.append(pitch_shifted)
                except:
                    pass
            
            # 4. ë³¼ë¥¨ ì¡°ì ˆ
            volume_factor = np.random.uniform(0.7, 1.3)
            volume_adjusted = audio_data * volume_factor
            augmented_data.append(volume_adjusted)
            
            # 5. ì‹œê°„ ì´ë™ (Time Shifting)
            shift_samples = np.random.randint(0, len(audio_data) // 4)
            shifted_audio = np.roll(audio_data, shift_samples)
            augmented_data.append(shifted_audio)
        
        return augmented_data
    
    def batch_augment_data(self, audio_files: List[str], labels: List[int], 
                          augmentation_factor: int = 3) -> Tuple[List[np.ndarray], List[int]]:
        """ë°°ì¹˜ ë°ì´í„° ì¦ê°•"""
        augmented_audio = []
        augmented_labels = []
        
        for audio_file, label in zip(audio_files, labels):
            try:
                audio_data, sr = librosa.load(audio_file, sr=16000)
                augmented_samples = self.augment_audio_data(audio_data, sr, augmentation_factor)
                
                augmented_audio.extend(augmented_samples)
                augmented_labels.extend([label] * len(augmented_samples))
                
            except Exception as e:
                print(f"âŒ íŒŒì¼ ì¦ê°• ì˜¤ë¥˜ {audio_file}: {e}")
                continue
        
        return augmented_audio, augmented_labels
    
    # ===== 2. ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ (Advanced Feature Extraction) =====
    
    def extract_advanced_features(self, audio_data: np.ndarray, sr: int) -> Dict[str, float]:
        """ê³ ê¸‰ ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ"""
        features = {}
        
        try:
            # 1. MFCC (Mel-frequency cepstral coefficients)
            mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)
            features['mfcc_mean'] = np.mean(mfccs, axis=1).tolist()
            features['mfcc_std'] = np.std(mfccs, axis=1).tolist()
            features['mfcc_delta'] = np.mean(librosa.feature.delta(mfccs), axis=1).tolist()
            
            # 2. Chroma (ìŒê³„ íŠ¹ì§•)
            chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)
            features['chroma_mean'] = np.mean(chroma, axis=1).tolist()
            features['chroma_std'] = np.std(chroma, axis=1).tolist()
            
            # 3. Spectral Contrast
            contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr)
            features['contrast_mean'] = np.mean(contrast, axis=1).tolist()
            features['contrast_std'] = np.std(contrast, axis=1).tolist()
            
            # 4. Tonnetz (ì¡°ì„± íŠ¹ì§•)
            tonnetz = librosa.feature.tonnetz(y=audio_data, sr=sr)
            features['tonnetz_mean'] = np.mean(tonnetz, axis=1).tolist()
            
            # 5. Zero Crossing Rate (ê³ ê¸‰)
            zcr = librosa.feature.zero_crossing_rate(audio_data)
            features['zcr_mean'] = np.mean(zcr)
            features['zcr_std'] = np.std(zcr)
            
            # 6. Spectral Rolloff (ê³ ê¸‰)
            rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sr)
            features['rolloff_mean'] = np.mean(rolloff)
            features['rolloff_std'] = np.std(rolloff)
            
            # 7. Spectral Bandwidth (ê³ ê¸‰)
            bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=sr)
            features['bandwidth_mean'] = np.mean(bandwidth)
            features['bandwidth_std'] = np.std(bandwidth)
            
            # 8. Spectral Centroid (ê³ ê¸‰)
            centroid = librosa.feature.spectral_centroid(y=audio_data, sr=sr)
            features['centroid_mean'] = np.mean(centroid)
            features['centroid_std'] = np.std(centroid)
            
            # 9. RMS Energy (ê³ ê¸‰)
            rms = librosa.feature.rms(y=audio_data)
            features['rms_mean'] = np.mean(rms)
            features['rms_std'] = np.std(rms)
            
            # 10. Spectral Flatness
            flatness = librosa.feature.spectral_flatness(y=audio_data)
            features['flatness_mean'] = np.mean(flatness)
            features['flatness_std'] = np.std(flatness)
            
            # 11. Tempo (í…œí¬)
            tempo, _ = librosa.beat.beat_track(y=audio_data, sr=sr)
            features['tempo'] = tempo if tempo is not None else 0.0
            
            # 12. Spectral Contrast (ê³ ê¸‰)
            contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr)
            features['contrast_mean'] = np.mean(contrast)
            features['contrast_std'] = np.std(contrast)
            
            # 13. Poly Features (ë‹¤í•­ì‹ íŠ¹ì§•)
            poly_features = librosa.feature.poly_features(y=audio_data, sr=sr)
            features['poly_mean'] = np.mean(poly_features)
            features['poly_std'] = np.std(poly_features)
            
            # 14. Tonnetz (ê³ ê¸‰)
            tonnetz = librosa.feature.tonnetz(y=audio_data, sr=sr)
            features['tonnetz_mean'] = np.mean(tonnetz)
            features['tonnetz_std'] = np.std(tonnetz)
            
            # 15. Chroma CQT
            chroma_cqt = librosa.feature.chroma_cqt(y=audio_data, sr=sr)
            features['chroma_cqt_mean'] = np.mean(chroma_cqt, axis=1).tolist()
            
        except Exception as e:
            print(f"âŒ ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            features = {f'feature_{i}': 0.0 for i in range(50)}
        
        return features
    
    # ===== 3. íŠ¹ì§• ì„ íƒ (Feature Selection) =====
    
    def select_best_features(self, X: np.ndarray, y: np.ndarray, 
                           method: str = 'mutual_info', k: int = 20) -> np.ndarray:
        """
        ìµœì  íŠ¹ì§• ì„ íƒ
        
        Args:
            X: íŠ¹ì§• í–‰ë ¬
            y: ë ˆì´ë¸”
            method: ì„ íƒ ë°©ë²• ('mutual_info', 'f_score', 'variance')
            k: ì„ íƒí•  íŠ¹ì§• ìˆ˜
            
        Returns:
            ì„ íƒëœ íŠ¹ì§• ì¸ë±ìŠ¤
        """
        if method == 'mutual_info':
            selector = SelectKBest(score_func=mutual_info_classif, k=k)
        elif method == 'f_score':
            selector = SelectKBest(score_func=f_classif, k=k)
        else:
            from sklearn.feature_selection import VarianceThreshold
            selector = VarianceThreshold(threshold=0.01)
        
        X_selected = selector.fit_transform(X, y)
        self.feature_selector = selector
        
        # íŠ¹ì§• ì¤‘ìš”ë„ ì €ì¥
        if hasattr(selector, 'scores_'):
            self.feature_importance = {
                'scores': selector.scores_,
                'selected_features': selector.get_support(indices=True)
            }
        
        return X_selected
    
    # ===== 4. ê³ ê¸‰ ì•™ìƒë¸” ë°©ë²• =====
    
    def create_advanced_ensemble(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±"""
        
        # ë‹¤ì–‘í•œ ëª¨ë¸ë“¤
        models = {
            'random_forest': RandomForestClassifier(
                n_estimators=100, 
                max_depth=10, 
                random_state=42
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100, 
                learning_rate=0.1, 
                random_state=42
            ),
            'extra_trees': RandomForestClassifier(
                n_estimators=100, 
                max_depth=10, 
                random_state=42,
                criterion='entropy'
            )
        }
        
        # ê° ëª¨ë¸ í›ˆë ¨
        trained_models = {}
        for name, model in models.items():
            model.fit(X, y)
            trained_models[name] = model
            print(f"âœ… {name} ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        
        self.models = trained_models
        return trained_models
    
    def stacking_ensemble_predict(self, X: np.ndarray) -> np.ndarray:
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì˜ˆì¸¡"""
        if not self.models:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡
        base_predictions = []
        for name, model in self.models.items():
            pred_proba = model.predict_proba(X)
            base_predictions.append(pred_proba)
        
        # 2ë‹¨ê³„: ë©”íƒ€ í•™ìŠµê¸° (ê°„ë‹¨í•œ í‰ê· )
        meta_features = np.hstack(base_predictions)
        final_prediction = np.mean(meta_features, axis=1)
        
        return final_prediction
    
    # ===== 5. ì•¡í‹°ë¸Œ í•™ìŠµ (Active Learning) =====
    
    def active_learning_query(self, X: np.ndarray, model, 
                            query_strategy: str = 'uncertainty') -> np.ndarray:
        """
        ì•¡í‹°ë¸Œ í•™ìŠµì„ ìœ„í•œ ì¿¼ë¦¬ ìƒ˜í”Œ ì„ íƒ
        
        Args:
            X: íŠ¹ì§• í–‰ë ¬
            model: í›ˆë ¨ëœ ëª¨ë¸
            query_strategy: ì¿¼ë¦¬ ì „ëµ ('uncertainty', 'diversity', 'random')
            
        Returns:
            ì„ íƒëœ ìƒ˜í”Œ ì¸ë±ìŠ¤
        """
        if query_strategy == 'uncertainty':
            # ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ì„ íƒ
            pred_proba = model.predict_proba(X)
            uncertainty = 1 - np.max(pred_proba, axis=1)
            query_indices = np.argsort(uncertainty)[-10:]  # ìƒìœ„ 10ê°œ
            
        elif query_strategy == 'diversity':
            # ë‹¤ì–‘ì„± ê¸°ë°˜ ì„ íƒ (í´ëŸ¬ìŠ¤í„°ë§)
            kmeans = KMeans(n_clusters=10, random_state=42)
            cluster_labels = kmeans.fit_predict(X)
            
            # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ í•˜ë‚˜ì”© ì„ íƒ
            query_indices = []
            for cluster_id in range(10):
                cluster_mask = cluster_labels == cluster_id
                if np.any(cluster_mask):
                    cluster_indices = np.where(cluster_mask)[0]
                    query_indices.append(np.random.choice(cluster_indices))
            
        else:  # random
            # ëœë¤ ì„ íƒ
            query_indices = np.random.choice(len(X), size=10, replace=False)
        
        return query_indices
    
    # ===== 6. ë‹¤ì¤‘ ì‹œê°„ ìŠ¤ì¼€ì¼ ë¶„ì„ =====
    
    def multi_timescale_analysis(self, audio_data: np.ndarray, sr: int) -> Dict[str, Dict]:
        """ë‹¤ì¤‘ ì‹œê°„ ìŠ¤ì¼€ì¼ ë¶„ì„"""
        
        results = {}
        
        # ë‹¨ê¸° ë¶„ì„ (1ì´ˆ)
        short_window = int(1.0 * sr)
        if len(audio_data) >= short_window:
            short_audio = audio_data[:short_window]
            results['short_term'] = self.extract_advanced_features(short_audio, sr)
        
        # ì¤‘ê¸° ë¶„ì„ (5ì´ˆ)
        medium_window = int(5.0 * sr)
        if len(audio_data) >= medium_window:
            medium_audio = audio_data[:medium_window]
            results['medium_term'] = self.extract_advanced_features(medium_audio, sr)
        
        # ì¥ê¸° ë¶„ì„ (ì „ì²´)
        results['long_term'] = self.extract_advanced_features(audio_data, sr)
        
        # ì‹œê°„ì  ë³€í™” ë¶„ì„
        if len(audio_data) >= medium_window:
            # ìœˆë„ìš°ë³„ ë¶„ì„
            window_size = int(1.0 * sr)  # 1ì´ˆ ìœˆë„ìš°
            windows = [audio_data[i:i+window_size] for i in range(0, len(audio_data)-window_size, window_size)]
            
            window_features = []
            for window in windows:
                if len(window) == window_size:
                    features = self.extract_advanced_features(window, sr)
                    window_features.append(features)
            
            if window_features:
                # ì‹œê°„ì  ë³€í™” í†µê³„
                temporal_stats = {}
                for key in window_features[0].keys():
                    if isinstance(window_features[0][key], (int, float)):
                        values = [f[key] for f in window_features]
                        temporal_stats[f'{key}_temporal_mean'] = np.mean(values)
                        temporal_stats[f'{key}_temporal_std'] = np.std(values)
                        temporal_stats[f'{key}_temporal_trend'] = np.polyfit(range(len(values)), values, 1)[0]
                
                results['temporal_analysis'] = temporal_stats
        
        return results
    
    # ===== 7. ë² ì´ì§€ì•ˆ ìµœì í™” =====
    
    def bayesian_optimization(self, X: np.ndarray, y: np.ndarray, 
                            param_space: Dict) -> Dict:
        """ë² ì´ì§€ì•ˆ ìµœì í™” (ê°„ë‹¨í•œ ê·¸ë¦¬ë“œ ì„œì¹˜)"""
        
        best_score = 0
        best_params = {}
        
        # ê°„ë‹¨í•œ ê·¸ë¦¬ë“œ ì„œì¹˜ (ì‹¤ì œë¡œëŠ” Gaussian Process ì‚¬ìš©)
        for n_estimators in param_space.get('n_estimators', [50, 100, 200]):
            for max_depth in param_space.get('max_depth', [5, 10, 15]):
                for learning_rate in param_space.get('learning_rate', [0.01, 0.1, 0.2]):
                    
                    # ëª¨ë¸ í›ˆë ¨
                    model = GradientBoostingClassifier(
                        n_estimators=n_estimators,
                        max_depth=max_depth,
                        learning_rate=learning_rate,
                        random_state=42
                    )
                    
                    # êµì°¨ ê²€ì¦
                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
                    model.fit(X_train, y_train)
                    score = model.score(X_test, y_test)
                    
                    if score > best_score:
                        best_score = score
                        best_params = {
                            'n_estimators': n_estimators,
                            'max_depth': max_depth,
                            'learning_rate': learning_rate,
                            'score': score
                        }
        
        return best_params
    
    # ===== 8. í†µí•© ì •í™•ë„ í–¥ìƒ íŒŒì´í”„ë¼ì¸ =====
    
    def comprehensive_improvement_pipeline(self, audio_files: List[str], 
                                         labels: List[int]) -> Dict:
        """ì¢…í•©ì ì¸ ì •í™•ë„ í–¥ìƒ íŒŒì´í”„ë¼ì¸"""
        
        print("ğŸš€ ì¢…í•© ì •í™•ë„ í–¥ìƒ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        
        # 1. ë°ì´í„° ì¦ê°•
        print("1ï¸âƒ£ ë°ì´í„° ì¦ê°• ì¤‘...")
        augmented_audio, augmented_labels = self.batch_augment_data(
            audio_files, labels, augmentation_factor=2
        )
        
        # 2. ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ
        print("2ï¸âƒ£ ê³ ê¸‰ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        features_list = []
        for audio_data in augmented_audio:
            features = self.extract_advanced_features(audio_data, 16000)
            # ë”•ì…”ë„ˆë¦¬ë¥¼ í‰íƒ„í™”
            flat_features = []
            for key, value in features.items():
                if isinstance(value, list):
                    flat_features.extend(value)
                else:
                    flat_features.append(value)
            features_list.append(flat_features)
        
        X = np.array(features_list)
        y = np.array(augmented_labels)
        
        # 3. íŠ¹ì§• ì„ íƒ
        print("3ï¸âƒ£ íŠ¹ì§• ì„ íƒ ì¤‘...")
        X_selected = self.select_best_features(X, y, method='mutual_info', k=30)
        
        # 4. ê³ ê¸‰ ì•™ìƒë¸”
        print("4ï¸âƒ£ ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        models = self.create_advanced_ensemble(X_selected, y)
        
        # 5. ë² ì´ì§€ì•ˆ ìµœì í™”
        print("5ï¸âƒ£ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì¤‘...")
        param_space = {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, 15],
            'learning_rate': [0.01, 0.1, 0.2]
        }
        best_params = self.bayesian_optimization(X_selected, y, param_space)
        
        # 6. ìµœì¢… ëª¨ë¸ í›ˆë ¨
        print("6ï¸âƒ£ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        final_model = GradientBoostingClassifier(**best_params, random_state=42)
        final_model.fit(X_selected, y)
        
        # 7. ì„±ëŠ¥ í‰ê°€
        print("7ï¸âƒ£ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)
        final_model.fit(X_train, y_train)
        y_pred = final_model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        results = {
            'accuracy': accuracy,
            'best_params': best_params,
            'feature_importance': self.feature_importance,
            'selected_features_count': X_selected.shape[1],
            'total_samples': len(augmented_audio),
            'original_samples': len(audio_files)
        }
        
        print(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ì •í™•ë„: {accuracy:.3f}")
        return results

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    # ì •í™•ë„ í–¥ìƒ ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸
    improver = AccuracyImprovementMethods()
    
    print("ğŸ¯ ì •í™•ë„ í–¥ìƒ ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ê°€ìƒì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_audio_files = ["test1.wav", "test2.wav", "test3.wav"]
    test_labels = [0, 1, 0]  # 0: ì •ìƒ, 1: ì´ìƒ
    
    # ì¢…í•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    results = improver.comprehensive_improvement_pipeline(test_audio_files, test_labels)
    
    print("ğŸ“Š ê²°ê³¼:")
    print(f"   ì •í™•ë„: {results['accuracy']:.3f}")
    print(f"   ì„ íƒëœ íŠ¹ì§• ìˆ˜: {results['selected_features_count']}")
    print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {results['total_samples']}")
    print(f"   ì›ë³¸ ìƒ˜í”Œ ìˆ˜: {results['original_samples']}")
