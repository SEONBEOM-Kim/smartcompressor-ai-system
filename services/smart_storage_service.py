"""
ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ì €ì¥ ì„œë¹„ìŠ¤
- AI ë¶„ì„ ê²°ê³¼ë§Œ ì €ì¥ (ì •ìƒ ì œì™¸, ì£¼ì˜/ê¸´ê¸‰ë§Œ)
- íŠ¹ì§• ë°ì´í„° ì••ì¶• ì €ì¥ (MFCC ë“±)
- ê¸ì •ì  ì‹ í˜¸ ìš”ì•½ ì €ì¥
"""

import sqlite3
import json
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging

class SmartStorageService:
    def __init__(self, db_path: str = "data/smart_storage.db"):
        self.db_path = db_path
        self.logger = logging.getLogger(__name__)
        self._init_database()
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        import os
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # AI ë¶„ì„ ê²°ê³¼ í…Œì´ë¸” (ì£¼ì˜/ê¸´ê¸‰ë§Œ ì €ì¥)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ai_analysis_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    store_id TEXT NOT NULL,
                    device_id TEXT NOT NULL,
                    analysis_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    diagnosis_type TEXT NOT NULL,  -- 'warning', 'critical'
                    confidence REAL NOT NULL,
                    is_overload BOOLEAN NOT NULL,
                    risk_level TEXT NOT NULL,  -- 'low', 'medium', 'high', 'critical'
                    features_json TEXT NOT NULL,  -- ì••ì¶•ëœ íŠ¹ì§• ë°ì´í„°
                    quality_metrics_json TEXT,  -- í’ˆì§ˆ ì§€í‘œ
                    noise_info_json TEXT,  -- ë…¸ì´ì¦ˆ ì •ë³´
                    processing_time_ms INTEGER,
                    file_size_bytes INTEGER,
                    file_hash TEXT,  -- íŒŒì¼ í•´ì‹œ (ì¤‘ë³µ ë°©ì§€)
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # íŠ¹ì§• ë°ì´í„° í…Œì´ë¸” (ì••ì¶• ì €ì¥)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS compressed_features (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    analysis_id INTEGER,
                    feature_type TEXT NOT NULL,  -- 'mfcc', 'spectral', 'temporal'
                    compressed_data BLOB NOT NULL,  -- ì••ì¶•ëœ íŠ¹ì§• ë°ì´í„°
                    original_shape TEXT NOT NULL,  -- ì›ë³¸ shape ì •ë³´
                    compression_ratio REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (analysis_id) REFERENCES ai_analysis_results (id)
                )
            ''')
            
            # ê¸ì •ì  ì‹ í˜¸ ìš”ì•½ í…Œì´ë¸” (í•˜ë£¨ 1-2íšŒ)
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS positive_summaries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    store_id TEXT NOT NULL,
                    summary_date DATE NOT NULL,
                    total_analyses INTEGER DEFAULT 0,
                    normal_count INTEGER DEFAULT 0,
                    avg_confidence REAL,
                    avg_quality_score REAL,
                    peak_quality_score REAL,
                    summary_message TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(store_id, summary_date)
                )
            ''')
            
            # ì¸ë±ìŠ¤ ìƒì„±
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_analysis_store_time ON ai_analysis_results(store_id, analysis_timestamp)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_analysis_diagnosis ON ai_analysis_results(diagnosis_type, risk_level)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_positive_summary ON positive_summaries(store_id, summary_date)')
            
            conn.commit()
    
    def should_store_analysis(self, analysis_result: Dict) -> bool:
        """ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í• ì§€ íŒë‹¨"""
        # ì •ìƒì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì €ì¥ (ì£¼ì˜, ê¸´ê¸‰)
        if analysis_result.get('is_overload', False):
            return True
        
        # ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš° (ì£¼ì˜)
        confidence = analysis_result.get('confidence', 0)
        if confidence < 0.8:
            return True
        
        # í’ˆì§ˆì´ ë§¤ìš° ë‚®ì€ ê²½ìš°
        quality_metrics = analysis_result.get('quality_metrics', {})
        overall_quality = quality_metrics.get('overall_quality', 100)
        if overall_quality < 30:
            return True
        
        return False
    
    def compress_features(self, features: Dict) -> Dict:
        """íŠ¹ì§• ë°ì´í„° ì••ì¶•"""
        compressed = {}
        
        for key, value in features.items():
            if isinstance(value, (list, np.ndarray)):
                # NumPy ë°°ì—´ë¡œ ë³€í™˜
                if isinstance(value, list):
                    value = np.array(value)
                
                # ì••ì¶• (ê°„ë‹¨í•œ ì–‘ìí™”)
                if value.dtype == np.float64:
                    # 32ë¹„íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ìš©ëŸ‰ ì ˆë°˜ìœ¼ë¡œ
                    compressed_value = value.astype(np.float32)
                else:
                    compressed_value = value
                
                compressed[key] = {
                    'data': compressed_value.tolist(),
                    'shape': list(compressed_value.shape),
                    'dtype': str(compressed_value.dtype)
                }
            else:
                compressed[key] = value
        
        return compressed
    
    def store_analysis_result(self, store_id: str, device_id: str, 
                            analysis_result: Dict, file_info: Dict = None) -> Optional[int]:
        """AI ë¶„ì„ ê²°ê³¼ ì €ì¥ (ì£¼ì˜/ê¸´ê¸‰ë§Œ)"""
        
        if not self.should_store_analysis(analysis_result):
            self.logger.info(f"ì •ìƒ ë¶„ì„ ê²°ê³¼ - ì €ì¥í•˜ì§€ ì•ŠìŒ: {store_id}")
            return None
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # ìœ„í—˜ë„ ê³„ì‚°
                risk_level = self._calculate_risk_level(analysis_result)
                diagnosis_type = 'critical' if risk_level == 'critical' else 'warning'
                
                # íŠ¹ì§• ë°ì´í„° ì••ì¶•
                compressed_features = self.compress_features(
                    analysis_result.get('features', {})
                )
                
                # íŒŒì¼ í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ë°©ì§€)
                file_hash = self._calculate_file_hash(file_info) if file_info else None
                
                cursor.execute('''
                    INSERT INTO ai_analysis_results 
                    (store_id, device_id, diagnosis_type, confidence, is_overload, 
                     risk_level, features_json, quality_metrics_json, noise_info_json,
                     processing_time_ms, file_size_bytes, file_hash)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    store_id,
                    device_id,
                    diagnosis_type,
                    analysis_result.get('confidence', 0),
                    analysis_result.get('is_overload', False),
                    risk_level,
                    json.dumps(compressed_features),
                    json.dumps(analysis_result.get('quality_metrics', {})),
                    json.dumps(analysis_result.get('noise_info', {})),
                    analysis_result.get('processing_time_ms', 0),
                    file_info.get('size', 0) if file_info else 0,
                    file_hash
                ))
                
                analysis_id = cursor.lastrowid
                
                # ì••ì¶•ëœ íŠ¹ì§• ë°ì´í„° ì €ì¥
                self._store_compressed_features(cursor, analysis_id, compressed_features)
                
                conn.commit()
                
                self.logger.info(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {store_id} - {diagnosis_type}")
                return analysis_id
                
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def _calculate_risk_level(self, analysis_result: Dict) -> str:
        """ìœ„í—˜ë„ ê³„ì‚°"""
        confidence = analysis_result.get('confidence', 0)
        is_overload = analysis_result.get('is_overload', False)
        quality_metrics = analysis_result.get('quality_metrics', {})
        overall_quality = quality_metrics.get('overall_quality', 100)
        
        if is_overload and confidence > 0.9:
            return 'critical'
        elif is_overload or confidence < 0.6 or overall_quality < 20:
            return 'high'
        elif confidence < 0.8 or overall_quality < 40:
            return 'medium'
        else:
            return 'low'
    
    def _calculate_file_hash(self, file_info: Dict) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        import hashlib
        content = f"{file_info.get('name', '')}{file_info.get('size', 0)}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _store_compressed_features(self, cursor, analysis_id: int, features: Dict):
        """ì••ì¶•ëœ íŠ¹ì§• ë°ì´í„° ì €ì¥"""
        for feature_type, data in features.items():
            if isinstance(data, dict) and 'data' in data:
                compressed_data = json.dumps(data).encode('utf-8')
                original_shape = json.dumps(data.get('shape', []))
                compression_ratio = len(compressed_data) / (len(str(data)) * 8) if data else 1.0
                
                cursor.execute('''
                    INSERT INTO compressed_features 
                    (analysis_id, feature_type, compressed_data, original_shape, compression_ratio)
                    VALUES (?, ?, ?, ?, ?)
                ''', (analysis_id, feature_type, compressed_data, original_shape, compression_ratio))
    
    def update_positive_summary(self, store_id: str, analysis_result: Dict):
        """ê¸ì •ì  ì‹ í˜¸ ìš”ì•½ ì—…ë°ì´íŠ¸"""
        today = datetime.now().date()
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # ì˜¤ëŠ˜ì˜ ìš”ì•½ ë°ì´í„° ì¡°íšŒ
                cursor.execute('''
                    SELECT * FROM positive_summaries 
                    WHERE store_id = ? AND summary_date = ?
                ''', (store_id, today))
                
                existing = cursor.fetchone()
                
                if existing:
                    # ê¸°ì¡´ ìš”ì•½ ì—…ë°ì´íŠ¸
                    cursor.execute('''
                        UPDATE positive_summaries 
                        SET total_analyses = total_analyses + 1,
                            normal_count = normal_count + 1,
                            avg_confidence = (avg_confidence * total_analyses + ?) / (total_analyses + 1),
                            avg_quality_score = (avg_quality_score * total_analyses + ?) / (total_analyses + 1),
                            peak_quality_score = MAX(peak_quality_score, ?)
                        WHERE store_id = ? AND summary_date = ?
                    ''', (
                        analysis_result.get('confidence', 0),
                        analysis_result.get('quality_metrics', {}).get('overall_quality', 0),
                        analysis_result.get('quality_metrics', {}).get('overall_quality', 0),
                        store_id, today
                    ))
                else:
                    # ìƒˆë¡œìš´ ìš”ì•½ ìƒì„±
                    quality_metrics = analysis_result.get('quality_metrics', {})
                    summary_message = self._generate_positive_message(analysis_result)
                    
                    cursor.execute('''
                        INSERT INTO positive_summaries 
                        (store_id, summary_date, total_analyses, normal_count, 
                         avg_confidence, avg_quality_score, peak_quality_score, summary_message)
                        VALUES (?, ?, 1, 1, ?, ?, ?, ?)
                    ''', (
                        store_id, today,
                        analysis_result.get('confidence', 0),
                        quality_metrics.get('overall_quality', 0),
                        quality_metrics.get('overall_quality', 0),
                        summary_message
                    ))
                
                conn.commit()
                
        except Exception as e:
            self.logger.error(f"ê¸ì •ì  ìš”ì•½ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _generate_positive_message(self, analysis_result: Dict) -> str:
        """ê¸ì •ì  ë©”ì‹œì§€ ìƒì„±"""
        quality_metrics = analysis_result.get('quality_metrics', {})
        overall_quality = quality_metrics.get('overall_quality', 0)
        
        if overall_quality > 80:
            return "ì˜¤ëŠ˜ë„ ëƒ‰ë™ê³  ìƒíƒœê°€ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤! ğŸ‰"
        elif overall_quality > 60:
            return "ëƒ‰ë™ê³ ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤. ğŸ‘"
        else:
            return "ëƒ‰ë™ê³  ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. âš ï¸"
    
    def get_analysis_history(self, store_id: str, days: int = 7) -> List[Dict]:
        """ë¶„ì„ ì´ë ¥ ì¡°íšŒ"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                start_date = datetime.now() - timedelta(days=days)
                
                cursor.execute('''
                    SELECT * FROM ai_analysis_results 
                    WHERE store_id = ? AND analysis_timestamp >= ?
                    ORDER BY analysis_timestamp DESC
                ''', (store_id, start_date))
                
                results = []
                for row in cursor.fetchall():
                    results.append({
                        'id': row[0],
                        'store_id': row[1],
                        'device_id': row[2],
                        'analysis_timestamp': row[3],
                        'diagnosis_type': row[4],
                        'confidence': row[5],
                        'is_overload': bool(row[6]),
                        'risk_level': row[7],
                        'features': json.loads(row[8]) if row[8] else {},
                        'quality_metrics': json.loads(row[9]) if row[9] else {},
                        'noise_info': json.loads(row[10]) if row[10] else {},
                        'processing_time_ms': row[11],
                        'file_size_bytes': row[12]
                    })
                
                return results
                
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_positive_summaries(self, store_id: str, days: int = 7) -> List[Dict]:
        """ê¸ì •ì  ìš”ì•½ ì¡°íšŒ"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                start_date = datetime.now() - timedelta(days=days)
                
                cursor.execute('''
                    SELECT * FROM positive_summaries 
                    WHERE store_id = ? AND summary_date >= ?
                    ORDER BY summary_date DESC
                ''', (store_id, start_date))
                
                results = []
                for row in cursor.fetchall():
                    results.append({
                        'id': row[0],
                        'store_id': row[1],
                        'summary_date': row[2],
                        'total_analyses': row[3],
                        'normal_count': row[4],
                        'avg_confidence': row[5],
                        'avg_quality_score': row[6],
                        'peak_quality_score': row[7],
                        'summary_message': row[8],
                        'created_at': row[9]
                    })
                
                return results
                
        except Exception as e:
            self.logger.error(f"ê¸ì •ì  ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_storage_stats(self) -> Dict:
        """ì €ì¥ì†Œ í†µê³„ ì¡°íšŒ"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # ë¶„ì„ ê²°ê³¼ í†µê³„
                cursor.execute('SELECT COUNT(*) FROM ai_analysis_results')
                total_analyses = cursor.fetchone()[0]
                
                cursor.execute('SELECT COUNT(*) FROM ai_analysis_results WHERE diagnosis_type = "critical"')
                critical_count = cursor.fetchone()[0]
                
                cursor.execute('SELECT COUNT(*) FROM ai_analysis_results WHERE diagnosis_type = "warning"')
                warning_count = cursor.fetchone()[0]
                
                # ì••ì¶• íŠ¹ì§• ë°ì´í„° í†µê³„
                cursor.execute('SELECT COUNT(*) FROM compressed_features')
                compressed_features_count = cursor.fetchone()[0]
                
                # ê¸ì •ì  ìš”ì•½ í†µê³„
                cursor.execute('SELECT COUNT(*) FROM positive_summaries')
                positive_summaries_count = cursor.fetchone()[0]
                
                return {
                    'total_analyses': total_analyses,
                    'critical_count': critical_count,
                    'warning_count': warning_count,
                    'compressed_features_count': compressed_features_count,
                    'positive_summaries_count': positive_summaries_count,
                    'storage_efficiency': f"{(total_analyses - compressed_features_count) / max(total_analyses, 1) * 100:.1f}%"
                }
                
        except Exception as e:
            self.logger.error(f"ì €ì¥ì†Œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
